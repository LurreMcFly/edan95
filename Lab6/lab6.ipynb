{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 6\n",
    "\n",
    "In and after this lab session you will\n",
    "\n",
    "    - train a Gaussian NBC with the EM algorithm\n",
    "    - compare the results you get to those of the k-Means clustering provided in SciKitLearn\n",
    "    - discuss the classifiers from this lab session and those from the previous session (supervised learning of NBCs) in a \n",
    "    brief report\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported  1797  number of images from MNIST dataset\n",
      "Split into a 70/30 ratio between train and test set\n",
      "Number of training data  (1257, 64)\n",
      "Number of test data  (540, 64)\n"
     ]
    }
   ],
   "source": [
    "mnist = load_digits()\n",
    "print(\"Imported \", len(mnist.data), \" number of images from MNIST dataset\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(mnist.data, mnist.target,test_size = 0.3)\n",
    "\n",
    "print(\"Split into a 70/30 ratio between train and test set\")\n",
    "print(\"Number of training data \", x_train.shape)\n",
    "print(\"Number of test data \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from numpy import linalg as npl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS NOT NAIVE!\n",
    "\n",
    "class EM_alg():\n",
    "    def __init__(self, n_clusters, epsilon = 1e-7):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.mu = {}\n",
    "        self.std = {}\n",
    "        self.p_C = {}\n",
    "        self.eps = epsilon\n",
    "        self.n_features = 0\n",
    "        self.r = {}\n",
    "        self.N = 0\n",
    "\n",
    "    \n",
    "    def log_prob(self, x, label):\n",
    "        prob = np.sum(-1/2*np.log(npl.det(self.std[label])+self.eps) - \\\n",
    "                1/2*np.inner(x-self.mu[label],np.matmul(npl.pinv(self.std[label] + np.eye(self.n_features)*self.eps),x-self.mu[label])))\n",
    "        return prob\n",
    "        \n",
    "        \n",
    "    \n",
    "    def E_step(self, X):\n",
    "        for point in range(self.N):\n",
    "       # for label in range(self.n_clusters):\n",
    "            self.r[label] = np.zeros(self.N)\n",
    "            nominator = np.sum([np.log(self.p_C[k] + self.eps)+self.log_prob(X[point,:],k) for k in range(self.n_clusters)])\n",
    "            # for point in range(self.N):\n",
    "            for label in range(self.n_clusters):\n",
    "                nominator = np.sum([self.p_C[k]*+np.exp(elf.log_prob(X[point,:],k)) for k in range(self.n_clusters)])\n",
    "                self.r[label][point] = self.p_C[label]*np.exp(self.log_prob(X[point,:],label))/nominator\n",
    "            \n",
    "        \n",
    "    def M_step(self, X):\n",
    "        for label in range(self.n_clusters):\n",
    "            r_k = np.sum([self.r[label][i] for i in range(self.N)])\n",
    "            self.p_C[label] = r_k/self.N\n",
    "            self.mu[label] = np.sum([self.r[label][i]*X[i] for i in range(self.N)],axis = 0)/r_k\n",
    "            top = np.zeros((self.n_features,self.n_features))\n",
    "            for i in range(self.N):\n",
    "                top += self.r[label][i]*X[i,:]*np.transpose(X[i,:])\n",
    "            self.std[label] = top/r_k - np.outer(self.mu[label],self.mu[label])\n",
    "        \n",
    "\n",
    "    def fit(self, X):    \n",
    "        self.n_features = X.shape[1]\n",
    "        self.N = X.shape[0]\n",
    "        for label in range(self.n_clusters):\n",
    "            self.mu[label] = np.random.rand(self.n_features)*np.max(X)\n",
    "            self.std[label] = np.diag(np.random.rand(self.n_features))\n",
    "            \n",
    "            self.p_C[label]= 1/self.n_clusters\n",
    "            \n",
    "        for i in range(10):\n",
    "            self.E_step(X)\n",
    "            self.M_step(X)\n",
    "                    \n",
    "            \n",
    "    def predict(self, X_new):\n",
    "        yhat = np.zeros(X_new.shape[0])\n",
    "        for point in range(X_new.shape[0]):\n",
    "            yhat[point] = np.argmax([np.log(self.p_C[k] + self.eps)+self.log_prob(X_new[point,:],k) for k in range(self.n_clusters)])\n",
    "\n",
    "        return yhat\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EM_alg(10)\n",
    "em.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.mu\n",
    "predictions = em.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix own GNB:\n",
      "[[  0   6   0  35   0  83   0   0  11   1]\n",
      " [  8   3  67   8   2  10   2   0  28   0]\n",
      " [  0   3   0  46   2   2  60   3  13   2]\n",
      " [  0   7   0 117   1   0   0   0   0   2]\n",
      " [  0   1  26   0   0  90   0   0   1   2]\n",
      " [  1  14  46  44   2   9   0   1   1   1]\n",
      " [  0   0   1   0   1  75   0   0  50   0]\n",
      " [  1   0  49  23   0   3  18   1   8  32]\n",
      " [  1   6  25  28   7  19   5   0  10  12]\n",
      " [  0   3   0  98   1   1   2   0  11   5]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix own GNB:\\n%s\" % metrics.confusion_matrix(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class own_KMeans():\n",
    "    def __init__(self, n_clusters, epsilon = 1e-5):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.eps = epsilon\n",
    "        self.p_C = {}\n",
    "        self.mu = {}\n",
    "    \n",
    "    def E_step(self, X):\n",
    "        for i,x in enumerate(X):\n",
    "            self.y[i] = np.argmin([npl.norm(x-self.mu[label]) for label in range(self.n_clusters)])\n",
    "       \n",
    "    def M_step(self, X):\n",
    "        for label in range(self.n_clusters):\n",
    "            Ci = self.y == label\n",
    "            if sum(np.array(Ci)) != 0:\n",
    "                self.mu[label] = np.sum(X[Ci,:], axis = 0)/sum(np.array(Ci))\n",
    "      \n",
    "\n",
    "    def fit(self, X):\n",
    "        self.y = np.zeros(X.shape[0]) \n",
    "        for label in range(self.n_clusters):\n",
    "            self.p_C[label] = 1/self.n_clusters\n",
    "            self.mu[label] = np.random.rand(X.shape[1])*np.max(X)\n",
    "        \n",
    "        for i in range(10):\n",
    "            self.E_step(X)\n",
    "            self.M_step(X)\n",
    "            \n",
    "    def get_labels(self):\n",
    "        return self.y\n",
    "            \n",
    "    def predict(self,X_new):\n",
    "        yhat = np.zeros(X_new.shape[0])\n",
    "        for i,x in enumerate(X_new):\n",
    "            yhat[i] = np.argmin([npl.norm(x-self.mu[label]) for label in range(self.n_clusters)])\n",
    "        \n",
    "        return yhat\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_kmeans = own_KMeans(10)\n",
    "own_kmeans.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own_kmeans.mu\n",
    "predictions = own_kmeans.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 6, ..., 8, 0, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix own GNB:\n",
      "[[ 69   0   0   0   0   0  66   0   0   1]\n",
      " [  0   0   2  35   1  20   0   0  70   0]\n",
      " [  0   0   0   0   7 109   0  10   5   0]\n",
      " [  0   0   0   1   4   0   0 115   7   0]\n",
      " [  0   0   0   0   5   0   0   0   2 113]\n",
      " [  0   0   0  19   0   0   0  43  56   1]\n",
      " [  0   0 126   0   0   0   0   0   1   0]\n",
      " [  0   0   0   4 125   0   0   0   6   0]\n",
      " [  0   0   1  14   2   2   0  29  65   0]\n",
      " [  0   0   0  19   3   0   0  97   2   0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix own GNB:\\n%s\" % metrics.confusion_matrix(y_train, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10).fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels from the kmeans algorithm:  [9 3 6 ... 1 5 6]\n",
      "Lables in y_train:  [9 8 5 ... 1 0 9]\n",
      "Cluster centers for the kmeans alg:  [[ 0.00000000e+00  5.52000000e-01  8.15200000e+00  1.42800000e+01\n",
      "   1.41440000e+01  7.17600000e+00  3.84000000e-01 -2.35922393e-16\n",
      "   1.60000000e-02  4.17600000e+00  1.27520000e+01  9.09600000e+00\n",
      "   1.10000000e+01  1.21840000e+01  1.66400000e+00 -1.38777878e-17\n",
      "   8.00000000e-03  2.32800000e+00  4.26400000e+00  3.56800000e+00\n",
      "   1.16320000e+01  9.68800000e+00  7.76000000e-01  9.02056208e-17\n",
      "  -1.62630326e-18  2.00000000e-01  9.84000000e-01  7.88000000e+00\n",
      "   1.38880000e+01  6.31200000e+00  2.64000000e-01 -3.25260652e-18\n",
      "   0.00000000e+00  8.80000000e-02  5.20000000e-01  4.00800000e+00\n",
      "   1.15440000e+01  1.19280000e+01  2.33600000e+00  0.00000000e+00\n",
      "  -1.90819582e-17  4.96000000e-01  1.41600000e+00  4.80000000e-01\n",
      "   4.02400000e+00  1.20400000e+01  6.58400000e+00  5.20417043e-17\n",
      "  -2.60208521e-17  8.16000000e-01  7.01600000e+00  6.49600000e+00\n",
      "   8.12000000e+00  1.30880000e+01  6.34400000e+00  1.44000000e-01\n",
      "   0.00000000e+00  4.48000000e-01  9.00800000e+00  1.48880000e+01\n",
      "   1.40560000e+01  8.98400000e+00  2.16000000e+00  3.76000000e-01]\n",
      " [ 0.00000000e+00  9.01639344e-01  1.01147541e+01  1.45327869e+01\n",
      "   7.87704918e+00  1.06557377e+00 -1.33226763e-15 -2.22044605e-16\n",
      "   1.63934426e-02  5.34426230e+00  1.37868852e+01  1.27213115e+01\n",
      "   1.17131148e+01  3.38524590e+00  4.88498131e-15 -1.38777878e-17\n",
      "   1.63934426e-02  4.68852459e+00  8.27049180e+00  5.97540984e+00\n",
      "   1.23606557e+01  3.67213115e+00 -3.55271368e-15  9.02056208e-17\n",
      "  -1.62630326e-18  9.59016393e-01  2.64754098e+00  6.81967213e+00\n",
      "   1.29180328e+01  2.43442623e+00 -2.22044605e-15 -3.25260652e-18\n",
      "   0.00000000e+00  5.73770492e-02  1.44262295e+00  9.83606557e+00\n",
      "   1.09016393e+01  1.43442623e+00 -6.66133815e-15  0.00000000e+00\n",
      "  -1.73472348e-17  1.14754098e-01  4.21311475e+00  1.14754098e+01\n",
      "   8.06557377e+00  2.67213115e+00  1.04098361e+00  1.63934426e-02\n",
      "  -2.60208521e-17  9.34426230e-01  1.09918033e+01  1.39754098e+01\n",
      "   1.24180328e+01  1.17786885e+01  7.88524590e+00  1.08196721e+00\n",
      "   0.00000000e+00  8.85245902e-01  1.04016393e+01  1.45901639e+01\n",
      "   1.32131148e+01  1.16639344e+01  8.58196721e+00  2.86885246e+00]\n",
      " [ 0.00000000e+00  5.55111512e-17  1.06400000e+00  1.11280000e+01\n",
      "   9.33600000e+00  1.52000000e+00  6.40000000e-02 -2.35922393e-16\n",
      "  -1.30104261e-17  6.40000000e-02  6.96800000e+00  1.45440000e+01\n",
      "   6.21600000e+00  8.64000000e-01  6.40000000e-02 -1.38777878e-17\n",
      "   4.33680869e-18  7.36000000e-01  1.21840000e+01  9.52000000e+00\n",
      "   9.84000000e-01  1.36000000e-01 -3.55271368e-15  9.02056208e-17\n",
      "  -1.62630326e-18  2.25600000e+00  1.33840000e+01  8.26400000e+00\n",
      "   4.01600000e+00  2.12800000e+00  1.28000000e-01 -3.25260652e-18\n",
      "   0.00000000e+00  3.36000000e+00  1.45120000e+01  1.25040000e+01\n",
      "   1.18000000e+01  1.03280000e+01  2.83200000e+00  0.00000000e+00\n",
      "  -1.90819582e-17  1.87200000e+00  1.43360000e+01  1.08960000e+01\n",
      "   5.63200000e+00  1.01120000e+01  9.14400000e+00  2.48000000e-01\n",
      "  -2.60208521e-17  2.08000000e-01  1.02080000e+01  1.26240000e+01\n",
      "   5.24000000e+00  1.13840000e+01  1.09120000e+01  6.00000000e-01\n",
      "   0.00000000e+00  0.00000000e+00  1.34400000e+00  1.06880000e+01\n",
      "   1.50960000e+01  1.32080000e+01  4.56800000e+00  1.12000000e-01]\n",
      " [ 0.00000000e+00  1.33757962e-01  4.05095541e+00  1.20191083e+01\n",
      "   1.24522293e+01  5.31210191e+00  4.64968153e-01 -2.08166817e-16\n",
      "   1.27388535e-02  9.93630573e-01  8.49681529e+00  1.35987261e+01\n",
      "   1.25859873e+01  9.78343949e+00  1.52866242e+00  0.00000000e+00\n",
      "   4.77048956e-18  1.42038217e+00  8.26114650e+00  1.18025478e+01\n",
      "   1.25159236e+01  9.24203822e+00  9.36305732e-01  1.04083409e-16\n",
      "  -1.84314369e-18  1.06369427e+00  7.04458599e+00  1.41847134e+01\n",
      "   1.40955414e+01  4.57961783e+00  1.21019108e-01 -3.68628739e-18\n",
      "   0.00000000e+00  7.32484076e-01  8.00636943e+00  1.49808917e+01\n",
      "   1.28343949e+01  2.23566879e+00  2.54777070e-02  0.00000000e+00\n",
      "  -3.64291930e-17  1.18471338e+00  1.02802548e+01  1.18407643e+01\n",
      "   1.26305732e+01  4.50955414e+00  2.48407643e-01  7.63278329e-17\n",
      "  -8.67361738e-18  8.08917197e-01  9.12101911e+00  1.12165605e+01\n",
      "   1.23121019e+01  6.07006369e+00  7.00636943e-01  6.36942675e-03\n",
      "   0.00000000e+00  1.08280255e-01  4.08917197e+00  1.22547771e+01\n",
      "   1.30445860e+01  4.75796178e+00  7.70700637e-01  1.27388535e-02]\n",
      " [ 0.00000000e+00  5.55111512e-17  3.07692308e-02  1.73846154e+00\n",
      "   1.11692308e+01  1.20923077e+01  3.70769231e+00  6.15384615e-02\n",
      "  -5.20417043e-18  6.15384615e-02  1.81538462e+00  8.67692308e+00\n",
      "   1.38307692e+01  1.24923077e+01  5.10769231e+00  1.53846154e-01\n",
      "   3.03576608e-18  1.70769231e+00  8.10769231e+00  1.24307692e+01\n",
      "   1.16461538e+01  1.27384615e+01  3.96923077e+00  7.69230769e-02\n",
      "  -6.50521303e-19  3.58461538e+00  1.23076923e+01  1.15076923e+01\n",
      "   1.26923077e+01  1.35846154e+01  2.38461538e+00 -1.30104261e-18\n",
      "   0.00000000e+00  2.44615385e+00  7.69230769e+00  8.04615385e+00\n",
      "   1.24461538e+01  1.27384615e+01  2.06153846e+00  0.00000000e+00\n",
      "   5.20417043e-18  6.00000000e-01  1.63076923e+00  3.64615385e+00\n",
      "   1.15538462e+01  1.23538462e+01  1.47692308e+00 -4.16333634e-17\n",
      "   0.00000000e+00  4.61538462e-02  2.61538462e-01  3.06153846e+00\n",
      "   1.21846154e+01  1.20153846e+01  1.80000000e+00  8.32667268e-17\n",
      "   0.00000000e+00  0.00000000e+00 -2.66453526e-15  1.83076923e+00\n",
      "   1.18615385e+01  1.09538462e+01  1.66153846e+00  2.77555756e-16]\n",
      " [ 0.00000000e+00  3.07692308e-02  4.09230769e+00  1.31307692e+01\n",
      "   1.09923077e+01  2.80769231e+00  2.30769231e-02 -2.63677968e-16\n",
      "  -1.30104261e-17  9.61538462e-01  1.27461538e+01  1.33153846e+01\n",
      "   1.16615385e+01  1.11307692e+01  7.76923077e-01 -1.38777878e-17\n",
      "   4.33680869e-18  3.83076923e+00  1.41153846e+01  5.20000000e+00\n",
      "   2.24615385e+00  1.24076923e+01  3.28461538e+00  9.02056208e-17\n",
      "  -1.62630326e-18  5.45384615e+00  1.24923077e+01  2.11538462e+00\n",
      "   1.53846154e-01  9.20769231e+00  6.42307692e+00 -3.25260652e-18\n",
      "   0.00000000e+00  5.96153846e+00  1.14307692e+01  9.00000000e-01\n",
      "   3.07692308e-02  8.65384615e+00  7.16923077e+00  0.00000000e+00\n",
      "  -2.25514052e-17  3.45384615e+00  1.33538462e+01  1.63846154e+00\n",
      "   1.31538462e+00  1.10846154e+01  5.94615385e+00  5.55111512e-17\n",
      "  -2.25514052e-17  7.23076923e-01  1.30769231e+01  9.99230769e+00\n",
      "   1.01461538e+01  1.34769231e+01  2.69230769e+00  3.07692308e-02\n",
      "   0.00000000e+00  0.00000000e+00  4.05384615e+00  1.36230769e+01\n",
      "   1.35384615e+01  5.73846154e+00  3.07692308e-01  2.30769231e-02]\n",
      " [ 0.00000000e+00  1.88235294e-01  6.92941176e+00  1.28647059e+01\n",
      "   1.17647059e+01  5.50000000e+00  5.23529412e-01 -1.11022302e-16\n",
      "   5.88235294e-03  2.69411765e+00  1.41117647e+01  9.20000000e+00\n",
      "   9.34117647e+00  9.64705882e+00  1.07058824e+00  1.38777878e-16\n",
      "   4.77048956e-18  4.27647059e+00  1.30529412e+01  4.36470588e+00\n",
      "   6.75882353e+00  1.06352941e+01  1.86470588e+00  1.04083409e-16\n",
      "  -7.58941521e-19  2.54705882e+00  1.10176471e+01  1.19823529e+01\n",
      "   1.28764706e+01  1.20529412e+01  2.62941176e+00 -1.51788304e-18\n",
      "   0.00000000e+00  5.29411765e-01  3.87647059e+00  6.64117647e+00\n",
      "   6.82941176e+00  1.14411765e+01  4.52941176e+00  0.00000000e+00\n",
      "  -4.16333634e-17  2.23529412e-01  2.39411765e+00  2.03529412e+00\n",
      "   1.68823529e+00  1.12823529e+01  6.32941176e+00  2.35294118e-02\n",
      "  -3.46944695e-18  8.88235294e-01  7.90588235e+00  5.16470588e+00\n",
      "   4.87647059e+00  1.29352941e+01  5.41764706e+00  7.64705882e-02\n",
      "   0.00000000e+00  1.64705882e-01  6.75294118e+00  1.38058824e+01\n",
      "   1.43647059e+01  9.42941176e+00  1.74705882e+00  2.35294118e-02]\n",
      " [ 0.00000000e+00  5.55111512e-17  2.90598291e-01  7.18803419e+00\n",
      "   1.18632479e+01  2.00000000e+00  2.13675214e-01  7.69230769e-02\n",
      "  -1.21430643e-17  8.54700855e-03  3.08547009e+00  1.38974359e+01\n",
      "   8.49572650e+00  1.77777778e+00  1.01709402e+00  2.82051282e-01\n",
      "   4.33680869e-18  5.98290598e-01  1.06923077e+01  1.15811966e+01\n",
      "   4.65811966e+00  5.37606838e+00  3.41025641e+00  2.64957265e-01\n",
      "   8.54700855e-03  4.72649573e+00  1.47521368e+01  5.74358974e+00\n",
      "   7.35897436e+00  1.02735043e+01  5.77777778e+00  1.70940171e-02\n",
      "   0.00000000e+00  8.93162393e+00  1.46837607e+01  9.27350427e+00\n",
      "   1.30683761e+01  1.42649573e+01  5.58119658e+00  0.00000000e+00\n",
      "   1.11111111e-01  7.00854701e+00  1.20683761e+01  1.27777778e+01\n",
      "   1.50341880e+01  1.06581197e+01  1.66666667e+00  4.51028104e-17\n",
      "   8.54700855e-02  1.25641026e+00  3.00854701e+00  7.89743590e+00\n",
      "   1.40769231e+01  3.84615385e+00  1.70940171e-02 -2.49800181e-16\n",
      "   0.00000000e+00  3.41880342e-02  3.24786325e-01  7.87179487e+00\n",
      "   1.24957265e+01  1.73504274e+00  3.55271368e-15  1.66533454e-16]\n",
      " [ 0.00000000e+00  1.27368421e+00  1.04947368e+01  1.35157895e+01\n",
      "   1.40210526e+01  1.26526316e+01  4.80000000e+00  4.21052632e-02\n",
      "   1.05263158e-02  5.04210526e+00  1.49894737e+01  1.28947368e+01\n",
      "   9.14736842e+00  7.37894737e+00  2.64210526e+00  3.15789474e-02\n",
      "   3.90312782e-18  6.52631579e+00  1.48105263e+01  6.18947368e+00\n",
      "   1.89473684e+00  8.31578947e-01  1.57894737e-01  6.93889390e-17\n",
      "  -1.30104261e-18  5.20000000e+00  1.45368421e+01  1.26947368e+01\n",
      "   7.31578947e+00  1.54736842e+00  1.05263158e-01 -2.60208521e-18\n",
      "   0.00000000e+00  1.73684211e+00  7.74736842e+00  1.03789474e+01\n",
      "   1.05789474e+01  4.55789474e+00  2.10526316e-01  0.00000000e+00\n",
      "   5.20417043e-18  3.78947368e-01  1.38947368e+00  5.40000000e+00\n",
      "   1.08736842e+01  6.03157895e+00  2.52631579e-01  1.73472348e-17\n",
      "  -1.73472348e-17  9.89473684e-01  5.68421053e+00  1.04736842e+01\n",
      "   1.21368421e+01  4.06315789e+00  1.05263158e-01 -2.77555756e-17\n",
      "   0.00000000e+00  1.29473684e+00  1.14842105e+01  1.45789474e+01\n",
      "   6.61052632e+00  6.52631579e-01  1.77635684e-15  6.66133815e-16]\n",
      " [ 0.00000000e+00  1.65562914e-01  4.87417219e+00  1.29668874e+01\n",
      "   1.40066225e+01  1.08807947e+01  4.41721854e+00  8.01324503e-01\n",
      "  -1.38777878e-17  1.13245033e+00  1.11059603e+01  1.16026490e+01\n",
      "   1.04503311e+01  1.26754967e+01  5.00662252e+00  5.49668874e-01\n",
      "   4.77048956e-18  1.21854305e+00  5.84768212e+00  2.47682119e+00\n",
      "   7.19867550e+00  1.14569536e+01  3.15894040e+00  9.93377483e-02\n",
      "  -1.73472348e-18  1.00662252e+00  5.41721854e+00  6.76158940e+00\n",
      "   1.24503311e+01  1.18543046e+01  4.88079470e+00 -3.46944695e-18\n",
      "   0.00000000e+00  1.49668874e+00  8.41721854e+00  1.28344371e+01\n",
      "   1.45033113e+01  1.03112583e+01  3.80132450e+00  0.00000000e+00\n",
      "  -3.29597460e-17  1.17880795e+00  5.09933775e+00  1.15496689e+01\n",
      "   1.08476821e+01  3.48344371e+00  5.56291391e-01  7.28583860e-17\n",
      "  -1.21430643e-17  1.05960265e-01  2.86754967e+00  1.26026490e+01\n",
      "   6.00662252e+00  3.37748344e-01  1.32450331e-02 -4.71844785e-16\n",
      "   0.00000000e+00  1.52317881e-01  6.11920530e+00  1.21523179e+01\n",
      "   2.31125828e+00  1.98675497e-01  3.31125828e-02 -3.33066907e-16]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels from the kmeans algorithm: \", kmeans.labels_)\n",
    "print(\"Lables in y_train: \", y_train)\n",
    "# print(\"Cluster centers for the kmeans alg: \", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGMM = BayesianGaussianMixture(n_components = 10)\n",
    "predictions = BGMM.fit_predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 3 6 ... 1 5 6]\n",
      "[6 9 0 ... 7 5 0]\n",
      "[9 8 5 ... 1 0 9]\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.labels_)\n",
    "print(predictions)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix own GNB:\n",
      "[[  0   0   0   0   0 129   0   0   0   0]\n",
      " [  0   1  39   0   0   0   0  16   0  68]\n",
      " [  0   0   2   0   8   1   0 105   0   4]\n",
      " [  3   0   0   1 113   0   4   0   0   9]\n",
      " [  0   0   5   0   0   0   6   0 116   2]\n",
      " [ 37   1   0  89   0   0   0   0   1   0]\n",
      " [  0 120   1   0   0   0   0   0   0   2]\n",
      " [  0   0   2   1   0   0 128   0   0   0]\n",
      " [ 36   2   6   5   2   0   3   1   0  72]\n",
      " [ 95   0  10   2   2   0   7   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix sklearn NBGMM:\\n%s\" % metrics.confusion_matrix(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Algorithm according to Lab description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from numpy import linalg as npl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_NaiveEM_alg():\n",
    "    def __init__(self, n_clusters, epsilon = 1e-3, tol = 1e-3):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.mu = {}\n",
    "        self.std = {}\n",
    "        self.p_C = {}\n",
    "        self.eps = epsilon\n",
    "        self.n_features = 0\n",
    "        self.r = {}\n",
    "        self.N = 0\n",
    "        self.mu_old = {}\n",
    "        self.tol = tol\n",
    "\n",
    "    \n",
    "    def log_prob(self, x, label):\n",
    "        prob = np.sum([-1/2*np.log(2*np.pi*self.std[label][j] + self.eps) - (x[j] - self.mu[label][j])**2/(2*self.std[label][j] + self.eps) for j in range(len(x))])\n",
    "        return prob\n",
    "    \n",
    "    def prob(self, x, label):\n",
    "        prob = sum([-1/2*np.log(2*np.pi*self.std[label][j] + self.eps) - (x[j] - self.mu[label][j])**2/(2*self.std[label][j] + self.eps) for j in range(len(x))])\n",
    "        return np.exp(prob)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def E_step(self, X):\n",
    "        for point in range(self.N):\n",
    "            nominator = sum([self.p_C[k]*self.prob(X[point,:],k) for k in range(self.n_clusters)])\n",
    "            for label in range(self.n_clusters):\n",
    "                self.r[label][point] = (self.p_C[label])*self.prob(X[point,:],label)/nominator\n",
    "        \n",
    "    def M_step(self, X):\n",
    "        for label in range(self.n_clusters):\n",
    "            r_k = sum(self.r[label])\n",
    "            top = np.zeros(self.n_features)\n",
    "            for i in range(self.N):\n",
    "                top += self.r[label][i]*np.diag(np.outer(X[i,:],X[i,:]))\n",
    "            self.p_C[label] = r_k/self.N\n",
    "            self.mu_old[label] = self.mu[label]\n",
    "            self.mu[label] = np.sum([self.r[label][i]*X[i,:] for i in range(self.N)],axis = 0)/r_k\n",
    "            self.std[label] = top/r_k - np.diag(np.outer(self.mu[label],self.mu[label]))\n",
    "        \n",
    "\n",
    "    def fit(self, X):    \n",
    "        self.n_features = X.shape[1]\n",
    "        self.N = X.shape[0]\n",
    "#         indexes = np.random.choice(self.N, 10)\n",
    "        indexes = [i for i in range(self.N)]\n",
    "        batch = int(self.N/self.n_clusters)\n",
    "        np.random.shuffle(indexes)\n",
    "        for label in range(self.n_clusters):\n",
    "            self.r[label] = np.zeros(self.N)\n",
    "            idx = indexes[label*batch:(label+1)*batch]\n",
    "            self.mu_old[label] = np.zeros(self.n_features)\n",
    "            self.mu[label] = np.mean(X[idx,:], axis = 0)\n",
    "            self.std[label] = np.var(X[idx,:], axis = 0)\n",
    "            self.p_C[label]= 1/self.n_clusters\n",
    "            \n",
    "        for i in range(100):\n",
    "            print(\"On step : \", i)\n",
    "            e_time_s = time.monotonic()\n",
    "            self.E_step(X)\n",
    "            print(\"Time to do one E-step: \", time.monotonic()-e_time_s)\n",
    "            m_time_s = time.monotonic()\n",
    "            self.M_step(X)\n",
    "            print(\"Time to do one M-step: \", time.monotonic()-m_time_s)\n",
    "            change = 0\n",
    "            for label in range(self.n_clusters):\n",
    "                change += npl.norm(self.mu[label] - self.mu_old[label])\n",
    "            print(change)\n",
    "            if change < self.tol:\n",
    "                print(\"Reached the tolerance and therefore converged!\")\n",
    "                break\n",
    "            print(\"Prior : \", self.p_C)\n",
    "    \n",
    "    def fit_cont(self,X,nbr_iters=10):\n",
    "        for i in range(nbr_iters):\n",
    "            print(\"On step : \", i)\n",
    "            e_time_s = time.monotonic()\n",
    "            self.E_step(X)\n",
    "            print(\"Time to do one E-step: \", time.monotonic()-e_time_s)\n",
    "            m_time_s = time.monotonic()\n",
    "            self.M_step(X)\n",
    "            print(\"Time to do one M-step: \", time.monotonic()-m_time_s)\n",
    "            print(\"Prior : \", self.p_C)\n",
    "    \n",
    "        \n",
    "                    \n",
    "            \n",
    "    def predict(self, X_new):\n",
    "        yhat = np.zeros(X_new.shape[0])\n",
    "        for point in range(X_new.shape[0]):\n",
    "            yhat[point] = np.argmax([np.log(self.p_C[k] + self.eps)+self.log_prob(X_new[point,:],k) for k in range(self.n_clusters)])\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n = x_train/16.0\n",
    "# M = np.mean(x_train,axis = 0)\n",
    "# STD = np.std(x_train,axis = 0)\n",
    "# x_train_n = np.zeros(x_train.shape)\n",
    "# for i in range(x_train.shape[1]):\n",
    "#     if STD[i] != 0:\n",
    "#         x_train_n[:,i] = (x_train[:,i]-M[i])/STD[i]\n",
    "#     else:\n",
    "#         x_train_n[:,i] = x_train[:,i]-M[i]\n",
    "# np.std(x_train_n,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On step :  0\n",
      "Time to do one E-step:  33.60899999999674\n",
      "Time to do one M-step:  1.422000000020489\n",
      "9.8384239478358\n",
      "Prior :  {0: 0.01762973428693039, 1: 0.10678200333927156, 2: 0.029462124228553477, 3: 0.014423833929257923, 4: 0.597397256103865, 5: 0.0029413022811440714, 6: 0.01590795500557732, 7: 0.08730390491591182, 8: 0.103698105817284, 9: 0.024453780092205047}\n",
      "On step :  1\n",
      "Time to do one E-step:  35.79599999997299\n",
      "Time to do one M-step:  1.8440000000409782\n",
      "5.0896597664235355\n",
      "Prior :  {0: 0.012446713206311844, 1: 0.11185081497760846, 2: 0.04689887860632497, 3: 0.04694148871706949, 4: 0.4625277360615247, 5: 0.0033491383840795453, 6: 0.055980264103271594, 7: 0.12463558180592049, 8: 0.10454176943103824, 9: 0.030827614706850623}\n",
      "On step :  2\n",
      "Time to do one E-step:  44.59399999998277\n",
      "Time to do one M-step:  2.0619999999762513\n",
      "2.817368093962427\n",
      "Prior :  {0: 0.04105138135203715, 1: 0.12785341197796957, 2: 0.06487974438627936, 3: 0.04333019354064921, 4: 0.36550571486897554, 5: 0.0050804838096586745, 6: 0.06367639026645217, 7: 0.12023300895638904, 8: 0.13366774207699506, 9: 0.03472192876459437}\n",
      "On step :  3\n",
      "Time to do one E-step:  39.95400000002701\n",
      "Time to do one M-step:  2.5469999999622814\n",
      "1.50869125370819\n",
      "Prior :  {0: 0.057228498575452875, 1: 0.12921897753764644, 2: 0.07642460814649836, 3: 0.04001974403828316, 4: 0.31985333508270647, 5: 0.00612671761587613, 6: 0.06704374775202605, 7: 0.1329784989845196, 8: 0.13749799528234388, 9: 0.03360787698464694}\n",
      "On step :  4\n",
      "Time to do one E-step:  37.85899999999674\n",
      "Time to do one M-step:  2.3130000000237487\n",
      "0.9439181842066263\n",
      "Prior :  {0: 0.06566179715488049, 1: 0.12509176208555897, 2: 0.08727057338068765, 3: 0.03781633689513885, 4: 0.30257692900690397, 5: 0.006545538862835431, 6: 0.06893299005228529, 7: 0.13876825204627016, 8: 0.1341660430361395, 9: 0.033169777479299715}\n",
      "On step :  5\n",
      "Time to do one E-step:  32.42199999996228\n",
      "Time to do one M-step:  1.547000000020489\n",
      "1.0737748523866422\n",
      "Prior :  {0: 0.07322419672197263, 1: 0.11806761366602676, 2: 0.0976699302607131, 3: 0.03695510652512819, 4: 0.2886806158585718, 5: 0.007158949009581544, 6: 0.07014298308653748, 7: 0.14145958871686984, 8: 0.13158025554760444, 9: 0.03506076060699419}\n",
      "On step :  6\n",
      "Time to do one E-step:  34.85899999999674\n",
      "Time to do one M-step:  1.3910000000032596\n",
      "0.7604515589760125\n",
      "Prior :  {0: 0.07658041780459543, 1: 0.10843719289948964, 2: 0.10662632718148587, 3: 0.035973670289711955, 4: 0.27555202946455376, 5: 0.007211903046427866, 6: 0.07321388083143776, 7: 0.15033979797674446, 8: 0.1300933351508423, 9: 0.03597144535471094}\n",
      "On step :  7\n",
      "Time to do one E-step:  30.56199999997625\n",
      "Time to do one M-step:  1.9839999999967404\n",
      "0.6141342808353386\n",
      "Prior :  {0: 0.0790073729389214, 1: 0.1041043335003467, 2: 0.11414843024675198, 3: 0.03579230786475208, 4: 0.26513775134015016, 5: 0.007367141735155121, 6: 0.07772743803358534, 7: 0.15250633276096184, 8: 0.12797648765261127, 9: 0.03623240392676387}\n",
      "On step :  8\n",
      "Time to do one E-step:  40.65600000001723\n",
      "Time to do one M-step:  1.672000000020489\n",
      "0.825094231547465\n",
      "Prior :  {0: 0.0817255417069534, 1: 0.10088750341243345, 2: 0.12277759042303367, 3: 0.03555262457973996, 4: 0.25315164103915133, 5: 0.007901480839313685, 6: 0.08127423869848546, 7: 0.15158537492041044, 8: 0.12670310058616063, 9: 0.0384409037943176}\n",
      "On step :  9\n",
      "Time to do one E-step:  39.31199999997625\n",
      "Time to do one M-step:  2.3589999999967404\n",
      "0.5760245050960411\n",
      "Prior :  {0: 0.08424337299148243, 1: 0.09723760873792975, 2: 0.12829366267705491, 3: 0.03614478923986989, 4: 0.24615381067113504, 5: 0.008113309063130531, 6: 0.08466318087912646, 7: 0.15126782150623846, 8: 0.12619767058278988, 9: 0.03768477365124247}\n",
      "On step :  10\n",
      "Time to do one E-step:  37.40600000001723\n",
      "Time to do one M-step:  1.687999999965541\n",
      "0.576430255435098\n",
      "Prior :  {0: 0.08681086364056886, 1: 0.09405495968316815, 2: 0.13429758976315576, 3: 0.03731658716919175, 4: 0.2383667892299117, 5: 0.008800978831276576, 6: 0.08419553760393397, 7: 0.15314537398346026, 8: 0.12707104286207485, 9: 0.03594027723325785}\n",
      "On step :  11\n",
      "Time to do one E-step:  36.71799999999348\n",
      "Time to do one M-step:  1.6880000000237487\n",
      "0.546384611683044\n",
      "Prior :  {0: 0.08834518958251614, 1: 0.09044121269029225, 2: 0.13762950803731416, 3: 0.03833336685818378, 4: 0.23094502277443688, 5: 0.009979415024067356, 6: 0.08329079171306043, 7: 0.1571000177129678, 8: 0.12798753316015335, 9: 0.035947942447007636}\n",
      "On step :  12\n",
      "Time to do one E-step:  38.26500000001397\n",
      "Time to do one M-step:  1.6559999999590218\n",
      "0.510468783763343\n",
      "Prior :  {0: 0.08870954855011547, 1: 0.086778187843549, 2: 0.13949113323426757, 3: 0.038928990446660004, 4: 0.2232262715464847, 5: 0.012037691707955567, 6: 0.08255306335375692, 7: 0.16253787770905614, 8: 0.1293931400143607, 9: 0.036344095593793785}\n",
      "On step :  13\n",
      "Time to do one E-step:  39.79700000002049\n",
      "Time to do one M-step:  1.452999999979511\n",
      "0.45456493449285645\n",
      "Prior :  {0: 0.08922739609805709, 1: 0.08452624711703977, 2: 0.1394491283322, 3: 0.03906051445911329, 4: 0.21912389812115515, 5: 0.013718543174906993, 6: 0.08225894255468555, 7: 0.1661099545444186, 8: 0.12912688676487055, 9: 0.03739848883355281}\n",
      "On step :  14\n",
      "Time to do one E-step:  34.125\n",
      "Time to do one M-step:  1.4530000000377186\n",
      "0.5990486369273844\n",
      "Prior :  {0: 0.09001523796932005, 1: 0.08490832339592333, 2: 0.13981969816343837, 3: 0.03918214894066775, 4: 0.21561472575667764, 5: 0.01878220919343644, 6: 0.07845665556234689, 7: 0.16747629218062485, 8: 0.12783298165276272, 9: 0.037911727184801576}\n",
      "On step :  15\n",
      "Time to do one E-step:  40.35999999998603\n",
      "Time to do one M-step:  1.6089999999967404\n",
      "0.6055157811208098\n",
      "Prior :  {0: 0.09101424646446603, 1: 0.08320312840197172, 2: 0.1398212503687693, 3: 0.03937620870251393, 4: 0.2121541140733516, 5: 0.02352882860663688, 6: 0.07677437442579597, 7: 0.17121734850620907, 8: 0.1244826030186125, 9: 0.03842789743167315}\n",
      "On step :  16\n",
      "Time to do one E-step:  36.375\n",
      "Time to do one M-step:  1.4209999999729916\n",
      "0.7680234309569833\n",
      "Prior :  {0: 0.09107276124145779, 1: 0.08341797658646072, 2: 0.13857679067589287, 3: 0.03970091052876284, 4: 0.20999886858241637, 5: 0.030471943343324637, 6: 0.07548539578869029, 7: 0.1717165860194645, 8: 0.11952396371192865, 9: 0.04003480352160095}\n",
      "On step :  17\n",
      "Time to do one E-step:  37.06300000002375\n",
      "Time to do one M-step:  1.7660000000032596\n",
      "0.7803107848120111\n",
      "Prior :  {0: 0.09158758140536515, 1: 0.08400779065930272, 2: 0.13379295258214377, 3: 0.039442610312277186, 4: 0.20722456201146605, 5: 0.03545549870034726, 6: 0.08425067461630091, 7: 0.1700141966595869, 8: 0.11154528967300767, 9: 0.04267884338020213}\n",
      "On step :  18\n",
      "Time to do one E-step:  32.14000000001397\n",
      "Time to do one M-step:  1.3439999999827705\n",
      "0.6967754219548121\n",
      "Prior :  {0: 0.09329114826353052, 1: 0.08574683713450247, 2: 0.1295022774970687, 3: 0.039864022196505984, 4: 0.2044433059351595, 5: 0.04129493499586921, 6: 0.08250460061014321, 7: 0.17023128803459947, 8: 0.10827292572883207, 9: 0.04484865960378887}\n",
      "On step :  19\n",
      "Time to do one E-step:  33.18699999997625\n",
      "Time to do one M-step:  1.6560000000172295\n",
      "0.5728303573436734\n",
      "Prior :  {0: 0.09568975456790177, 1: 0.08644204226973644, 2: 0.12508486713663228, 3: 0.04017685100666665, 4: 0.20239385354238604, 5: 0.04508846223573406, 6: 0.07931949355200874, 7: 0.17028138464388387, 8: 0.10763444967432054, 9: 0.047888841370729476}\n",
      "On step :  20\n",
      "Time to do one E-step:  33.31300000002375\n",
      "Time to do one M-step:  1.4369999999762513\n",
      "0.3963991107845227\n",
      "Prior :  {0: 0.09703591094459786, 1: 0.08702905083695942, 2: 0.12287239337498948, 3: 0.04040365157326768, 4: 0.20013838863370045, 5: 0.048254514131192, 6: 0.0762368981894183, 7: 0.1726142555998509, 8: 0.10660055616325104, 9: 0.04881438055277262}\n",
      "On step :  21\n",
      "Time to do one E-step:  32.5\n",
      "Time to do one M-step:  1.422000000020489\n",
      "0.4419337664451106\n",
      "Prior :  {0: 0.09788620786984784, 1: 0.08806185851308115, 2: 0.12035128465691691, 3: 0.03970057745122176, 4: 0.19720476572001455, 5: 0.05361415895828323, 6: 0.07273647577243869, 7: 0.17433818443290627, 8: 0.10603092234327306, 9: 0.05007556428201658}\n",
      "On step :  22\n",
      "Time to do one E-step:  29.76600000000326\n",
      "Time to do one M-step:  1.3899999999557622\n",
      "0.493778453990016\n",
      "Prior :  {0: 0.0988516640710886, 1: 0.08938770188225992, 2: 0.11899029424616789, 3: 0.03822887124693029, 4: 0.19365163793361098, 5: 0.0587266386891281, 6: 0.06989548031411237, 7: 0.17492090735049398, 8: 0.10515256062340646, 9: 0.05219424364280143}\n",
      "On step :  23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to do one E-step:  38.46900000004098\n",
      "Time to do one M-step:  1.452999999979511\n",
      "0.3483143774769249\n",
      "Prior :  {0: 0.09886980903395286, 1: 0.09101750440422002, 2: 0.11732633318498335, 3: 0.037365931283068754, 4: 0.19085355617017738, 5: 0.06048646310813883, 6: 0.07128994430560144, 7: 0.17279074280053927, 8: 0.10547339598867723, 9: 0.054526319720640946}\n",
      "On step :  24\n",
      "Time to do one E-step:  36.92200000002049\n",
      "Time to do one M-step:  1.9219999999622814\n",
      "0.2744825247221391\n",
      "Prior :  {0: 0.09887992688681316, 1: 0.09348397333768531, 2: 0.11616579658558018, 3: 0.037294788366381414, 4: 0.1878174993808831, 5: 0.060564200685981014, 6: 0.0749291100521158, 7: 0.16870673577276624, 8: 0.1066410859377921, 9: 0.055516882994001475}\n",
      "On step :  25\n",
      "Time to do one E-step:  42.73499999998603\n",
      "Time to do one M-step:  2.25\n",
      "0.2528750221380366\n",
      "Prior :  {0: 0.09769171464339342, 1: 0.09669890997430992, 2: 0.11510308166610939, 3: 0.03725066803313112, 4: 0.1860587864237275, 5: 0.06061768397880166, 6: 0.07534584606019754, 7: 0.16628238419931426, 8: 0.1079487649250156, 9: 0.05700216009599964}\n",
      "On step :  26\n",
      "Time to do one E-step:  38.73499999998603\n",
      "Time to do one M-step:  2.0630000000237487\n",
      "0.18498395705901946\n",
      "Prior :  {0: 0.09724291923278357, 1: 0.09869434157374438, 2: 0.11400410290481781, 3: 0.037220477132576806, 4: 0.18425030779057078, 5: 0.06063747012500631, 6: 0.0763206919926772, 7: 0.1644966976718841, 8: 0.10966436322657101, 9: 0.05746862834936806}\n",
      "On step :  27\n",
      "Time to do one E-step:  37.68699999997625\n",
      "Time to do one M-step:  1.827999999979511\n",
      "0.1382629428256193\n",
      "Prior :  {0: 0.09675653622132349, 1: 0.09972338644268551, 2: 0.11341469326369295, 3: 0.037197290031236306, 4: 0.18116136296019106, 5: 0.060659799166688476, 6: 0.07618704922897067, 7: 0.16564043879733134, 8: 0.1113142789058075, 9: 0.0579451649820726}\n",
      "On step :  28\n",
      "Time to do one E-step:  38.73500000004424\n",
      "Time to do one M-step:  1.8589999999967404\n",
      "0.1067207451374823\n",
      "Prior :  {0: 0.09603719677421073, 1: 0.10080687281905165, 2: 0.11305158829283161, 3: 0.03718119217149767, 4: 0.178363802441203, 5: 0.06067554763497914, 6: 0.07634698557445756, 7: 0.16678343888311448, 8: 0.11268093057836072, 9: 0.05807244483029314}\n",
      "On step :  29\n",
      "Time to do one E-step:  37.64100000000326\n",
      "Time to do one M-step:  1.702999999979511\n",
      "0.0783094754175256\n",
      "Prior :  {0: 0.09551264130557177, 1: 0.10141507811821882, 2: 0.11267808471914251, 3: 0.03717034248885902, 4: 0.17586700000562636, 5: 0.06068629675464566, 6: 0.07637383322303595, 7: 0.16861291023697736, 8: 0.11361140923142606, 9: 0.058072403916496464}\n",
      "On step :  30\n",
      "Time to do one E-step:  47.375\n",
      "Time to do one M-step:  1.297000000020489\n",
      "0.07219283377098985\n",
      "Prior :  {0: 0.09543224004762771, 1: 0.10164396900614103, 2: 0.111855613422877, 3: 0.03716317840843568, 4: 0.17387042349180198, 5: 0.06069344064772385, 6: 0.07637466761754778, 7: 0.17069263919498193, 8: 0.11420197104312567, 9: 0.058071857119737426}\n",
      "On step :  31\n",
      "Time to do one E-step:  42.54699999996228\n",
      "Time to do one M-step:  1.5320000000065193\n",
      "0.06404221916088483\n",
      "Prior :  {0: 0.09541495042867136, 1: 0.10191591889379598, 2: 0.11140457417052549, 3: 0.037158514362700804, 4: 0.1712875957603803, 5: 0.06069804439948895, 6: 0.0763696413284344, 7: 0.17299102348048653, 8: 0.11468820381479808, 9: 0.058071533360717964}\n",
      "On step :  32\n",
      "Time to do one E-step:  34.96799999999348\n",
      "Time to do one M-step:  1.1410000000032596\n",
      "0.06322687196165004\n",
      "Prior :  {0: 0.09540799807011326, 1: 0.10204956035760317, 2: 0.11098525266580633, 3: 0.03715550868481137, 4: 0.16836655796897235, 5: 0.06070103979430788, 6: 0.076357651365449, 7: 0.1755961510001771, 8: 0.11530911134800383, 9: 0.0580711687447555}\n",
      "On step :  33\n",
      "Time to do one E-step:  31.79700000002049\n",
      "Time to do one M-step:  1.3899999999557622\n",
      "0.07940584713643672\n",
      "Prior :  {0: 0.0954081282406912, 1: 0.10215002619522762, 2: 0.110435148653177, 3: 0.03715358207730963, 4: 0.16475809086009696, 5: 0.06070295210817345, 6: 0.07635216137018018, 7: 0.17902360488349156, 8: 0.11594515268034852, 9: 0.05807115293130401}\n",
      "On step :  34\n",
      "Time to do one E-step:  30.219000000040978\n",
      "Time to do one M-step:  1.3899999999557622\n",
      "0.07360213167736562\n",
      "Prior :  {0: 0.09541285391237088, 1: 0.10231401270964544, 2: 0.11034474395792511, 3: 0.03715235334302272, 4: 0.16015172878041234, 5: 0.06070416034887615, 6: 0.0763502452142197, 7: 0.18351281415516918, 8: 0.11598578189231545, 9: 0.05807130568604305}\n",
      "On step :  35\n",
      "Time to do one E-step:  34.11000000004424\n",
      "Time to do one M-step:  1.327999999979511\n",
      "0.08285895583364829\n",
      "Prior :  {0: 0.095416657565315, 1: 0.10259531593102021, 2: 0.11023404491597864, 3: 0.03715157321798188, 4: 0.15511865080119577, 5: 0.06070493818280413, 6: 0.07634928498586283, 7: 0.18827798735754034, 8: 0.11607994563833372, 9: 0.05807160140396749}\n",
      "On step :  36\n",
      "Time to do one E-step:  38.90600000001723\n",
      "Time to do one M-step:  1.875\n",
      "0.09444882970584927\n",
      "Prior :  {0: 0.09540598359277246, 1: 0.10304384773320581, 2: 0.1100342040887015, 3: 0.03715107832549045, 4: 0.1496235001373344, 5: 0.060705431913686804, 6: 0.07634785400311853, 7: 0.19344354115245746, 8: 0.11617249214943598, 9: 0.058072066903796474}\n",
      "On step :  37\n",
      "Time to do one E-step:  46.20299999997951\n",
      "Time to do one M-step:  2.2959999999729916\n",
      "0.12956313744928263\n",
      "Prior :  {0: 0.0953388493835654, 1: 0.10365204657509142, 2: 0.10949690340229018, 3: 0.03715076490607211, 4: 0.14282937879122828, 5: 0.06070574467289101, 6: 0.07634495719914679, 7: 0.20013321119153155, 8: 0.11627552374113377, 9: 0.05807262013704932}\n",
      "On step :  38\n",
      "Time to do one E-step:  39.68800000002375\n",
      "Time to do one M-step:  1.4209999999729916\n",
      "0.1553459450197868\n",
      "Prior :  {0: 0.09519647311602793, 1: 0.10422058563566666, 2: 0.10826051991938775, 3: 0.037150566713021034, 4: 0.13587237617388875, 5: 0.06070594017028332, 6: 0.07634126687044841, 7: 0.20770363005750553, 8: 0.11647544056334788, 9: 0.05807320078042268}\n",
      "On step :  39\n",
      "Time to do one E-step:  33.64100000000326\n",
      "Time to do one M-step:  1.4060000000172295\n",
      "0.13979163963812616\n",
      "Prior :  {0: 0.09498570333346458, 1: 0.10482459953855698, 2: 0.10674117815703524, 3: 0.03715044176791791, 4: 0.131067241021121, 5: 0.06070605652887363, 6: 0.0763388185773574, 7: 0.2131840155081959, 8: 0.11692783934751921, 9: 0.05807410621995815}\n",
      "On step :  40\n",
      "Time to do one E-step:  37.29700000002049\n",
      "Time to do one M-step:  2.1410000000032596\n",
      "0.1681349869143636\n",
      "Prior :  {0: 0.09463506803389207, 1: 0.10598035251093671, 2: 0.10429810700302106, 3: 0.03715036358870913, 4: 0.12676912152688408, 5: 0.06070612661104328, 6: 0.07633937785104307, 7: 0.21900896019736746, 8: 0.11703689359709217, 9: 0.058075629080010856}\n",
      "On step :  41\n",
      "Time to do one E-step:  46.17199999996228\n",
      "Time to do one M-step:  1.672000000020489\n",
      "0.13981379716577857\n",
      "Prior :  {0: 0.09431306453215826, 1: 0.10732520405950075, 2: 0.10261128414392709, 3: 0.037150314999758086, 4: 0.12413125456924949, 5: 0.060706169734537754, 6: 0.07633873249046509, 7: 0.22277048897330987, 8: 0.11657339379220243, 9: 0.05808009270489113}\n",
      "On step :  42\n",
      "Time to do one E-step:  40.09399999998277\n",
      "Time to do one M-step:  1.6410000000032596\n",
      "0.10316665361413987\n",
      "Prior :  {0: 0.09399339575515037, 1: 0.10842255231021863, 2: 0.10260706434366412, 3: 0.037150284627264094, 4: 0.12110367229328059, 5: 0.0607062001395849, 6: 0.07633561452696346, 7: 0.22529685972162394, 8: 0.11630559972344788, 9: 0.058078756558801924}\n",
      "On step :  43\n",
      "Time to do one E-step:  42.60899999999674\n",
      "Time to do one M-step:  1.4369999999762513\n",
      "0.09776327814461241\n",
      "Prior :  {0: 0.0938849722771193, 1: 0.10871002900258758, 2: 0.10251582998564747, 3: 0.0371502660273941, 4: 0.1174352514644567, 5: 0.060706217498460585, 6: 0.0763352529469477, 7: 0.22996791336941505, 8: 0.11521429836964835, 9: 0.0580799690583232}\n",
      "On step :  44\n",
      "Time to do one E-step:  34.92200000002049\n",
      "Time to do one M-step:  1.5939999999827705\n",
      "0.12277568551980407\n",
      "Prior :  {0: 0.09387334155831215, 1: 0.10872155831911479, 2: 0.10172462949242064, 3: 0.03715025613046276, 4: 0.11341125624494362, 5: 0.06070623276803051, 6: 0.0767479346474386, 7: 0.234930986531205, 8: 0.11458538822367068, 9: 0.05814841608440112}\n",
      "On step :  45\n",
      "Time to do one E-step:  40.125\n",
      "Time to do one M-step:  1.6560000000172295\n",
      "0.14492392503115895\n",
      "Prior :  {0: 0.09384869334017823, 1: 0.10936325376916808, 2: 0.10099539803961036, 3: 0.03715026391435257, 4: 0.10910133446661097, 5: 0.06070631827798697, 6: 0.07709610394222402, 7: 0.23872360555696762, 8: 0.11424128867050759, 9: 0.0587737400223935}\n",
      "On step :  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to do one E-step:  40.56300000002375\n",
      "Time to do one M-step:  1.702999999979511\n",
      "0.13423382193257374\n",
      "Prior :  {0: 0.09372434595041038, 1: 0.10985467233270488, 2: 0.10086869250658846, 3: 0.0371502561205168, 4: 0.1045103162829536, 5: 0.06070705702190461, 6: 0.07760535353931096, 7: 0.24289703096475307, 8: 0.11380752695907348, 9: 0.05887474832178385}\n",
      "On step :  47\n",
      "Time to do one E-step:  39.21799999999348\n",
      "Time to do one M-step:  1.625\n",
      "0.1373492654658694\n",
      "Prior :  {0: 0.09326468895703663, 1: 0.11007259288917576, 2: 0.1006780219985629, 3: 0.037150384160677466, 4: 0.10176414992987963, 5: 0.06070705376054371, 6: 0.07793941761971884, 7: 0.24487628722474533, 8: 0.11388746200692826, 9: 0.059659941452731326}\n",
      "On step :  48\n",
      "Time to do one E-step:  43.01600000000326\n",
      "Time to do one M-step:  1.6709999999729916\n",
      "0.052545184673952645\n",
      "Prior :  {0: 0.0930825497376244, 1: 0.11018557587296081, 2: 0.10020921796361125, 3: 0.03715048574677818, 4: 0.10101116044897523, 5: 0.06070638810344237, 6: 0.07797526928769219, 7: 0.24582092872741304, 8: 0.11419496776112033, 9: 0.0596634563503821}\n",
      "On step :  49\n",
      "Time to do one E-step:  43.40700000000652\n",
      "Time to do one M-step:  1.9689999999827705\n",
      "0.056001908773818054\n",
      "Prior :  {0: 0.09306231198203128, 1: 0.11019227978068258, 2: 0.10000326747034505, 3: 0.03715045880169553, 4: 0.10022652073056454, 5: 0.06070640322290673, 6: 0.07806815508773898, 7: 0.24645757564109064, 8: 0.11447191435651073, 9: 0.05966111292643393}\n",
      "On step :  50\n",
      "Time to do one E-step:  41.40600000001723\n",
      "Time to do one M-step:  1.702999999979511\n",
      "0.038272201718094104\n",
      "Prior :  {0: 0.09303800065745511, 1: 0.11008181358642667, 2: 0.09925305807296385, 3: 0.03715043779925972, 4: 0.10023769391789783, 5: 0.060706425096940714, 6: 0.07828324385960152, 7: 0.2468746074257243, 8: 0.1147169572401507, 9: 0.059657762343579515}\n",
      "On step :  51\n",
      "Time to do one E-step:  45.70300000003772\n",
      "Time to do one M-step:  2.2660000000032596\n",
      "0.10610587148166974\n",
      "Prior :  {0: 0.09291498610572592, 1: 0.10977376046534405, 2: 0.09660212243632858, 3: 0.037150424926089436, 4: 0.10023863083660432, 5: 0.060706439665892675, 6: 0.0787605077282708, 7: 0.24868795541470523, 8: 0.11550408333448994, 9: 0.05966108908654913}\n",
      "On step :  52\n",
      "Time to do one E-step:  40.375\n",
      "Time to do one M-step:  2.51500000001397\n",
      "0.08417154850682118\n",
      "Prior :  {0: 0.09238744370651528, 1: 0.10970099155238588, 2: 0.09467411336810373, 3: 0.037150417264652735, 4: 0.10023866348016101, 5: 0.06070644645166391, 6: 0.07945596313038945, 7: 0.2501526544872236, 8: 0.11587002138903732, 9: 0.059663285169867006}\n",
      "On step :  53\n",
      "Time to do one E-step:  34.56300000002375\n",
      "Time to do one M-step:  1.4689999999827705\n",
      "0.05995207832664365\n",
      "Prior :  {0: 0.09228273235808727, 1: 0.10911132430358428, 2: 0.09387532571913787, 3: 0.037150412281469085, 4: 0.1002386634800373, 5: 0.060706451337435796, 6: 0.08035089246318682, 7: 0.2505639808640191, 8: 0.11605439333546032, 9: 0.05966582385758213}\n",
      "On step :  54\n",
      "Time to do one E-step:  32.26500000001397\n",
      "Time to do one M-step:  1.5309999999590218\n",
      "0.035159394759713684\n",
      "Prior :  {0: 0.09228550356891295, 1: 0.1090648654637137, 2: 0.09387065518614376, 3: 0.03715040935536364, 4: 0.10023866347594558, 5: 0.060706455422208164, 6: 0.08140864745673924, 7: 0.24930843246335338, 8: 0.11632436077750934, 9: 0.059642006830110304}\n",
      "On step :  55\n",
      "Time to do one E-step:  32.64100000000326\n",
      "Time to do one M-step:  1.4839999999967404\n",
      "0.041130974939965606\n",
      "Prior :  {0: 0.09228047623337501, 1: 0.108688021522719, 2: 0.09386919482888657, 3: 0.03715040775296956, 4: 0.10023866347550882, 5: 0.06070646626716481, 6: 0.08262693062644885, 7: 0.24822564630042293, 8: 0.11657697011165705, 9: 0.05963722288084732}\n",
      "On step :  56\n",
      "Time to do one E-step:  45.56200000003446\n",
      "Time to do one M-step:  2.125\n",
      "0.01095850950626417\n",
      "Prior :  {0: 0.09227162446911781, 1: 0.10869317989948536, 2: 0.09386904718924606, 3: 0.037150408270337144, 4: 0.10023866347531096, 5: 0.0607064658043746, 6: 0.0826598214251907, 7: 0.24795220085741795, 8: 0.1168223366499354, 9: 0.05963625195958379}\n",
      "On step :  57\n",
      "Time to do one E-step:  38.21899999998277\n",
      "Time to do one M-step:  1.3900000000139698\n",
      "0.015069490345824341\n",
      "Prior :  {0: 0.09226607085798512, 1: 0.10902673285927389, 2: 0.09386862020962004, 3: 0.03715040869648882, 4: 0.10023866347515104, 5: 0.06070646594875908, 6: 0.08235356423151653, 7: 0.24777253895207874, 8: 0.11698188474623533, 9: 0.05963505002289104}\n",
      "On step :  58\n",
      "Time to do one E-step:  41.96899999998277\n",
      "Time to do one M-step:  1.547000000020489\n",
      "0.016475147168356446\n",
      "Prior :  {0: 0.092261018238406, 1: 0.1094691624465909, 2: 0.0938682310768742, 3: 0.03715040902834203, 4: 0.10023866347503471, 5: 0.06070646558841081, 6: 0.08195115662276446, 7: 0.24763710323916713, 8: 0.11708401420955139, 9: 0.05963377607485843}\n",
      "On step :  59\n",
      "Time to do one E-step:  32.65599999995902\n",
      "Time to do one M-step:  1.547000000020489\n",
      "0.01727046837015836\n",
      "Prior :  {0: 0.09225441256589297, 1: 0.1098805433238613, 2: 0.09386771054180688, 3: 0.03715040921071681, 4: 0.10023866347491635, 5: 0.06070646515353048, 6: 0.08159144558329019, 7: 0.24752382443846283, 8: 0.11715407695880398, 9: 0.05963244874871829}\n",
      "On step :  60\n",
      "Time to do one E-step:  38.32799999997951\n",
      "Time to do one M-step:  1.5929999999934807\n",
      "0.014354826303451454\n",
      "Prior :  {0: 0.0922472324072411, 1: 0.1102627328104508, 2: 0.09386714235694454, 3: 0.03715040925402191, 4: 0.10023866347472511, 5: 0.06070646507148839, 6: 0.08128851387832621, 7: 0.2474009471333726, 8: 0.1172069525293758, 9: 0.05963094108405333}\n",
      "On step :  61\n",
      "Time to do one E-step:  41.23399999999674\n",
      "Time to do one M-step:  1.547000000020489\n",
      "0.004465777488342164\n",
      "Prior :  {0: 0.09223970949119685, 1: 0.11037367309440903, 2: 0.0938666635076427, 3: 0.03715040921395925, 4: 0.10023866347451199, 5: 0.060706464366934415, 6: 0.08123962698664387, 7: 0.24730239974408616, 8: 0.1172531166385908, 9: 0.059629273482024715}\n",
      "On step :  62\n",
      "Time to do one E-step:  41.09399999998277\n",
      "Time to do one M-step:  1.9679999999934807\n",
      "0.003388599306263005\n",
      "Prior :  {0: 0.09223525169628899, 1: 0.11045394223522409, 2: 0.09386662564377347, 3: 0.03715040902885267, 4: 0.1002386634743628, 5: 0.06070646398204978, 6: 0.08118703851045232, 7: 0.24724423810491716, 8: 0.1172891484706652, 9: 0.059628218853413364}\n",
      "On step :  63\n",
      "Time to do one E-step:  38.14100000000326\n",
      "Time to do one M-step:  1.5150000000139698\n",
      "0.0033137788746983718\n",
      "Prior :  {0: 0.09223296916456712, 1: 0.11053117498076367, 2: 0.09386665240118822, 3: 0.037150408773147146, 4: 0.10023866347428399, 5: 0.060706464384516136, 6: 0.08112094726376244, 7: 0.24721253815805816, 8: 0.11731270314825659, 9: 0.05962747825145642}\n",
      "On step :  64\n",
      "Time to do one E-step:  44.43800000002375\n",
      "Time to do one M-step:  2.389999999955762\n",
      "0.004386265993978978\n",
      "Prior :  {0: 0.09223140599783276, 1: 0.1106345638132131, 2: 0.09386681603623863, 3: 0.03715040859558285, 4: 0.10023866347424741, 5: 0.06070646468270057, 6: 0.08102273572602658, 7: 0.24719537663101343, 8: 0.11732668542895117, 9: 0.05962687961419318}\n",
      "On step :  65\n",
      "Time to do one E-step:  40.84400000004098\n",
      "Time to do one M-step:  1.562999999965541\n",
      "0.007415765892832132\n",
      "Prior :  {0: 0.09222900504275969, 1: 0.11081040644993426, 2: 0.09386724526653972, 3: 0.03715040845379132, 4: 0.10023866347422987, 5: 0.060706464870595674, 6: 0.08085092378969862, 7: 0.247185707458075, 8: 0.11733497158904792, 9: 0.059626203605327506}\n",
      "On step :  66\n",
      "Time to do one E-step:  36.81300000002375\n",
      "Time to do one M-step:  1.4219999999622814\n",
      "0.011036585656223884\n",
      "Prior :  {0: 0.09222248000679524, 1: 0.11107488240511315, 2: 0.09386844995157563, 3: 0.03715040828291851, 4: 0.10023866347422064, 5: 0.06070646503529815, 6: 0.08059311057147818, 7: 0.24717992752597978, 8: 0.11734052180744328, 9: 0.05962509093917716}\n",
      "On step :  67\n",
      "Time to do one E-step:  36.15600000001723\n",
      "Time to do one M-step:  1.577999999979511\n",
      "0.006123397847635899\n",
      "Prior :  {0: 0.09220436963242144, 1: 0.11123418826021572, 2: 0.09387182718183677, 3: 0.037150407995404595, 4: 0.1002386634742118, 5: 0.060706465397994964, 6: 0.08044887432476293, 7: 0.2471764626447666, 8: 0.1173454615006392, 9: 0.05962327958774588}\n",
      "On step :  68\n",
      "Time to do one E-step:  36.64000000001397\n",
      "Time to do one M-step:  1.422000000020489\n",
      "0.0022031378817663687\n",
      "Prior :  {0: 0.09217738163885074, 1: 0.11128841441609746, 2: 0.09387466584285466, 3: 0.03715040753296398, 4: 0.10023866347418921, 5: 0.06070646577952729, 6: 0.08041871127000016, 7: 0.24717395317770707, 8: 0.11734919134022884, 9: 0.059622145527580626}\n",
      "On step :  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to do one E-step:  37.375\n",
      "Time to do one M-step:  1.5309999999590218\n",
      "0.0017871740686564112\n",
      "Prior :  {0: 0.0921447206457053, 1: 0.11132655079686843, 2: 0.09387502756335421, 3: 0.037150407019440564, 4: 0.10023866347416101, 5: 0.060706465901831985, 6: 0.08041284980031649, 7: 0.24717239571592622, 8: 0.11735113018511327, 9: 0.05962178889728261}\n",
      "On step :  70\n",
      "Time to do one E-step:  45.79700000002049\n",
      "Time to do one M-step:  1.0310000000172295\n",
      "0.0022978589769845916\n",
      "Prior :  {0: 0.09209825678795079, 1: 0.1113741623841655, 2: 0.09387503855997233, 3: 0.03715040653844939, 4: 0.10023866347413261, 5: 0.060706465996637246, 6: 0.08041131188471488, 7: 0.2471719090458485, 8: 0.1173522619290409, 9: 0.0596215233990876}\n",
      "On step :  71\n",
      "Time to do one E-step:  40.65599999995902\n",
      "Time to do one M-step:  1.422000000020489\n",
      "0.00407443985844723\n",
      "Prior :  {0: 0.09201402848116404, 1: 0.11145844573025526, 2: 0.09387503578650201, 3: 0.037150406069445814, 4: 0.10023866347409621, 5: 0.060706466011095236, 6: 0.08041052774890771, 7: 0.247172252137573, 8: 0.11735299953369194, 9: 0.059621175027268665}\n",
      "On step :  72\n",
      "Time to do one E-step:  43.06199999997625\n",
      "Time to do one M-step:  1.6410000000032596\n",
      "0.009276631525010754\n",
      "Prior :  {0: 0.09182135858110556, 1: 0.11165068423072456, 2: 0.0938750377701611, 3: 0.037150405528073714, 4: 0.10023866347403278, 5: 0.06070646580089958, 6: 0.08040958832238801, 7: 0.24717364980509968, 8: 0.11735360843349955, 9: 0.059620538054015244}\n",
      "On step :  73\n",
      "Time to do one E-step:  38.71799999999348\n",
      "Time to do one M-step:  1.672000000020489\n",
      "0.01448709489184018\n",
      "Prior :  {0: 0.09152267571073522, 1: 0.11194832062064929, 2: 0.09387504461234192, 3: 0.037150404677663194, 4: 0.10023866347388799, 5: 0.06070646488510219, 6: 0.08040772495207713, 7: 0.24717731431617862, 8: 0.11735440087692206, 9: 0.05961898587444238}\n",
      "On step :  74\n",
      "Time to do one E-step:  48.82799999997951\n",
      "Time to do one M-step:  1.672000000020489\n",
      "0.0031631118146126435\n",
      "Prior :  {0: 0.09146711719620558, 1: 0.11200285276485329, 2: 0.09387505769423332, 3: 0.037150402933919004, 4: 0.10023866347365246, 5: 0.06070646258012726, 6: 0.08040480665489179, 7: 0.24718278949864395, 8: 0.11735553103117793, 9: 0.05961631617229546}\n",
      "On step :  75\n",
      "Time to do one E-step:  43.25\n",
      "Time to do one M-step:  1.5619999999762513\n",
      "0.0002610353625186533\n",
      "Reached the tolerance and therefore converged!\n"
     ]
    }
   ],
   "source": [
    "nem = my_NaiveEM_alg(10)\n",
    "nem.fit(x_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_n = x_test/16.0\n",
    "preds = nem.predict(x_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix own NBGMM:\n",
      "[[ 0  0  0  0 48  2  0  1  0  0]\n",
      " [ 0  4  0  0  0  0 25  0 13 21]\n",
      " [ 0  0  0  0  0  0  0  4 45  0]\n",
      " [ 0  5  0  0  0  0  0 42  4  0]\n",
      " [ 3  1  0 20  0 20  0  0  0  7]\n",
      " [ 0 34  0  0  0  0  0 20  0  1]\n",
      " [ 0  0 56  0  0  0  0  1  0  0]\n",
      " [47  1  0  0  0  0  0  1  0  4]\n",
      " [ 1  8  0  0  0  0 15 21  1  2]\n",
      " [ 7  2  0  0  0  0  0 44  2  7]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix own NBGMM:\\n%s\" % metrics.confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix looks bad so lets do some plotting and change the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "columns = 5\n",
    "rows = 2\n",
    "for i in range(10):\n",
    "    img = np.reshape(nem.mu[i],(8,8))\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(i)\n",
    "plt.show()\n",
    "# for i in range(10):\n",
    "#     img = np.reshape(nem.mu[i],(8,8))\n",
    "#     imgplot = plt.imshow(img)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_fix = np.zeros(len(preds))\n",
    "preds_fix[preds == 0] = 7\n",
    "preds_fix[preds == 1] = 5\n",
    "preds_fix[preds == 2] = 6\n",
    "preds_fix[preds == 3] = 8\n",
    "preds_fix[preds == 4] = 0\n",
    "preds_fix[preds == 5] = 4\n",
    "preds_fix[preds == 6] = 1\n",
    "preds_fix[preds == 7] = 3\n",
    "preds_fix[preds == 8] = 2\n",
    "preds_fix[preds == 9] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix own NBGMM:\n",
      "[[48  0  0  1  2  0  0  0  0  0]\n",
      " [ 0 25 13  0  0  4  0  0  0 21]\n",
      " [ 0  0 45  4  0  0  0  0  0  0]\n",
      " [ 0  0  4 42  0  5  0  0  0  0]\n",
      " [ 0  0  0  0 20  1  0  3 20  7]\n",
      " [ 0  0  0 20  0 34  0  0  0  1]\n",
      " [ 0  0  0  1  0  0 56  0  0  0]\n",
      " [ 0  0  0  1  0  1  0 47  0  4]\n",
      " [ 0 15  1 21  0  8  0  1  0  2]\n",
      " [ 0  0  2 44  0  2  0  7  0  7]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix own NBGMM:\\n%s\" % metrics.confusion_matrix(y_test, preds_fix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.11496070169349579, 1: 0.12445934647733116, 2: 0.0918861047768727, 3: 0.0731693304046471, 4: 0.11135414659426152, 5: 0.14593368295872408, 6: 0.09626120636436795, 7: 0.03046058477710074, 8: 0.12473930962915147, 9: 0.08677558632404732}\n",
      "[0.10103420843277645, 0.09944311853619729, 0.09864757358790771, 0.09069212410501193, 0.10103420843277645, 0.09864757358790771, 0.11535401750198886, 0.09785202863961814, 0.09864757358790771, 0.09864757358790771]\n"
     ]
    }
   ],
   "source": [
    "print(nem.p_C)\n",
    "label_dist = [sum(y_train==i)/len(y_train) for i in range(10)]\n",
    "print(label_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for othe new re-arranged labels: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        51\n",
      "           1       0.62      0.40      0.49        63\n",
      "           2       0.69      0.92      0.79        49\n",
      "           3       0.31      0.82      0.45        51\n",
      "           4       0.91      0.39      0.55        51\n",
      "           5       0.62      0.62      0.62        55\n",
      "           6       1.00      0.98      0.99        57\n",
      "           7       0.81      0.89      0.85        53\n",
      "           8       0.00      0.00      0.00        48\n",
      "           9       0.17      0.11      0.13        62\n",
      "\n",
      "    accuracy                           0.60       540\n",
      "   macro avg       0.61      0.61      0.58       540\n",
      "weighted avg       0.61      0.60      0.58       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for othe new re-arranged labels: \\n\", metrics.classification_report(y_test,preds_fix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
