{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDAN95 - Applied Machine Learning\n",
    "### LTH Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "### David Larsson & Jonas Lundgren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_GloVe(path = \"data/glove.6B.100d.txt\"):\n",
    "    \"\"\"\n",
    "    Function that reads GloVe embeddings and returns a dictionary, \n",
    "    where the keys will be the words and the values, the embeddings.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(path) as f:\n",
    "        for embeddings in f:\n",
    "            embeddings_list = embeddings.split()\n",
    "            d[embeddings_list[0]] = np.array([float(i) for i in embeddings_list[1:]])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = read_GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(word, glove_dict = glove_dict, num_closest = 5):\n",
    "    \"\"\"\n",
    "    Using a cosine similarity, computes the num_closest \n",
    "    closest words to word in glove_dict\n",
    "    \"\"\"\n",
    "    A = glove_dict[str(word)]\n",
    "    A_norm = np.linalg.norm(A, 2)\n",
    "    \n",
    "    cos_sims = np.zeros(len(glove_dict))\n",
    "    \n",
    "    for i, (k, v) in enumerate(glove_dict.items()):\n",
    "        B = v\n",
    "        B_norm = np.linalg.norm(B, 2)\n",
    "    \n",
    "        cos_sim = A.dot(B) / (A_norm * B_norm)\n",
    "        cos_sims[i] = cos_sim\n",
    "    \n",
    "    max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(word, glove_dict = glove_dict, num_closest = 5):\n",
    "    \"\"\"\n",
    "    Using a cosine similarity, computes the num_closest \n",
    "    closest words to word in glove_dict\n",
    "    \"\"\"\n",
    "    A = glove_dict[str(word)]\n",
    "    A_norm = np.linalg.norm(A, 2)\n",
    "    \n",
    "    cos_sims = np.zeros(len(glove_dict))\n",
    "    \n",
    "    for i, (k, v) in enumerate(glove_dict.items()):\n",
    "        if k != word:\n",
    "            B = v\n",
    "            B_norm = np.linalg.norm(B, 2)\n",
    "    \n",
    "            cos_sim = A.dot(B) / (A_norm * B_norm)\n",
    "            cos_sims[i] = cos_sim\n",
    "    \n",
    "    max_idxs = list(cos_sims.argsort()[-num_closest:])\n",
    "    \n",
    "    return [list(glove_dict.keys())[i] for i in max_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest 5 words to table : ['side', 'room', 'bottom', 'place', 'tables']\n",
      "Closest 5 words to table : ['paris', 'spain', 'britain', 'french', 'belgium']\n",
      "Closest 5 words to table : ['austria', 'netherlands', 'finland', 'norway', 'denmark']\n"
     ]
    }
   ],
   "source": [
    "print(\"Closest 5 words to table :\", cosine_sim('table'))\n",
    "print(\"Closest 5 words to table :\", cosine_sim('france'))\n",
    "print(\"Closest 5 words to table :\", cosine_sim('sweden'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'/home/jonas/projects/edan95/Lab4/data'\n",
    "\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + '/eng.train'\n",
    "    dev_file = BASE_DIR + '/eng.valid'\n",
    "    test_file = BASE_DIR + '/eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from course github: https://github.com/pnugues/edan95/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        rows = [row for row in rows if row[0] != '#']\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity recognition (NER) tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': 'O', 'ner': 'O'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return: X, Y\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should correspond with index 1 in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "X_train_cat, Y_train_cat = build_sequences(train_dict)\n",
    "X_dev_cat, Y_dev_cat = build_sequences(dev_dict)\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "print(X_train_cat[1])\n",
    "print(Y_train_cat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '$', '%', '&', \"'\", \"'d\", \"'ll\", \"'m\", \"'re\"]\n",
      "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_words = sorted(list(set([word for sentence in X_train_cat for word in sentence])))\n",
    "ner = sorted(list(set([ner for sentence in Y_train_cat for ner in sentence])))\n",
    "\n",
    "print(vocabulary_words[:10])\n",
    "print(ner[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '!!', '!!!', '!!!!', '!!!!!', '!?', '!?!', '\"', '#', '##']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_words = sorted(glove_dict.keys())\n",
    "embeddings_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '!!', '!!!', '!!!!', '!!!!!', '!?', '!?!', '\"', '#', '##']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words = sorted(list(set(vocabulary_words + embeddings_words)))\n",
    "vocabulary_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_words_reversed = dict(enumerate(vocabulary_words, start=2))\n",
    "tok_ners_reversed = dict(enumerate(ner, start=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: [('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11)]\n",
      "\n",
      "NER index: [('B-LOC', 2), ('B-MISC', 3), ('B-ORG', 4), ('I-LOC', 5), ('I-MISC', 6), ('I-ORG', 7), ('I-PER', 8), ('O', 9)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_words = {v: k for k, v in tok_words_reversed.items()}\n",
    "tokenized_ners = {v: k for k, v in tok_ners_reversed.items()}\n",
    "\n",
    "print('word index:', list(tokenized_words.items())[:10])\n",
    "print('\\nNER index:', list(tokenized_ners.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_idx(X, tokenized_x):\n",
    "    X_idxs = []\n",
    "    for sentence in X:\n",
    "        #Map unknowns to 1\n",
    "        X_idxs.append([tokenized_x[x] if x in tokenized_x else 1 for x in sentence])\n",
    "        \n",
    "    return X_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_idx = sentences_to_idx(X_train_cat, tokenized_words)\n",
    "Y_idx = sentences_to_idx(Y_train_cat, tokenized_ners)\n",
    "\n",
    "X_idx_dev = sentences_to_idx(X_dev_cat, tokenized_words)\n",
    "Y_idx_dev = sentences_to_idx(Y_dev_cat, tokenized_ners)\n",
    "\n",
    "X_idx_test = sentences_to_idx(X_test_cat, tokenized_words)\n",
    "Y_idx_test = sentences_to_idx(Y_test_cat, tokenized_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14987, 113)\n",
      "(14987, 113)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X_idx)\n",
    "Y = pad_sequences(Y_idx)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = pad_sequences(X_idx_dev, maxlen = max_len)\n",
    "Y_dev = pad_sequences(Y_idx_dev, maxlen = max_len)\n",
    "\n",
    "X_test = pad_sequences(X_idx_test, maxlen = max_len)\n",
    "Y_test = pad_sequences(Y_idx_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
