{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session 4 Edan 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GloVe embeddings 6B from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def readGloveEmbeddings(file):\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    word_dict = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embeddings = np.array([float(val) for val in splitLine[1:]])\n",
    "        word_dict[word] = embeddings\n",
    "    print(\"Number of words collected: \", len(word_dict))\n",
    "    f.close()\n",
    "    return word_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words collected:  400000\n"
     ]
    }
   ],
   "source": [
    "word_dict = readGloveEmbeddings('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of collecting embeddings given by Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words collected:  400000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "glove_dir = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4'\n",
    "embedding_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'UTF-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.strip().split()\n",
    "    word = values[0]\n",
    "    embeddings = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = embeddings\n",
    "f.close()\n",
    "print(\"Number of words collected: \", len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity # Raises error for 2D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as npl\n",
    "\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    sim = np.dot(a, b)/(npl.norm(a)*npl.norm(b))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top index:  [ 437 2389  927  241 7221 1801]  for word  table\n",
      "Top index:  [1035 1029  348  695 2975  387]  for word  france\n",
      "Top index:  [2640 2238 3817 3384 2819 2038]  for word  sweden\n"
     ]
    }
   ],
   "source": [
    "comparison_words = ['table', 'france', 'sweden']\n",
    "most_simular = {}\n",
    "\n",
    "for word in comparison_words:\n",
    "    sim = np.array([cos_sim(embedding_index[word], embedding_index[compare]) for compare in embedding_index.keys()])\n",
    "    top_idx = np.argpartition(sim, -6)[-6:]\n",
    "    print(\"Top index: \", top_idx[0:], \" for word \", word)\n",
    "    most_simular[word] = np.array([list(embedding_index.keys())[ind] for ind in top_idx[0:]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['side' 'bottom' 'room' 'place' 'tables' 'table']\n",
      "['paris' 'spain' 'french' 'britain' 'belgium' 'france']\n",
      "['austria' 'netherlands' 'finland' 'denmark' 'norway' 'sweden']\n"
     ]
    }
   ],
   "source": [
    "print(most_simular['table'])\n",
    "print(most_simular['france'])\n",
    "print(most_simular['sweden'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Corpus and Collecting Building Indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4'\n",
    "\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + '\\eng.train'\n",
    "    dev_file = BASE_DIR + '\\eng.valid'\n",
    "    test_file = BASE_DIR + '\\eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words ['-docstart-']\n",
      "First sentence, NER ['O']\n"
     ]
    }
   ],
   "source": [
    "X_train_cat, Y_train_cat = build_sequences(train_dict)\n",
    "X_dev_cat, Y_dev_cat = build_sequences(dev_dict)\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "print('First sentence, words', X_train_cat[0])\n",
    "print('First sentence, NER', Y_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_words = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_train_cat for word in sentence])))\n",
    "ner = sorted(list(set([ner for sentence \n",
    "                       in Y_train_cat for ner in sentence])))\n",
    "print(ner)\n",
    "NB_CLASSES = len(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file,encoding='UTF-8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in GloVe: 400000\n",
      "# unique words in the vocabulary: embeddings and corpus: 402597\n"
     ]
    }
   ],
   "source": [
    "embedding_file = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4\\glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)\n",
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "print('# unique words in the vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or NER lists) to indexes\n",
    "    :param X: List of word (or NER) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: [('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11)]\n",
      "NER index: [('B-LOC', 2), ('B-MISC', 3), ('B-ORG', 4), ('I-LOC', 5), ('I-MISC', 6), ('I-ORG', 7), ('I-PER', 8), ('O', 9)]\n",
      "First sentences, word indices [[935], [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936], [284434, 79019]]\n",
      "First sentences, NER indices [[9], [7, 9, 6, 9, 9, 9, 6, 9, 9], [8, 8]]\n"
     ]
    }
   ],
   "source": [
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "print('word index:', list(word_idx.items())[:10])\n",
    "print('NER index:', list(ner_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, ner_idx)\n",
    "X_idx_dev = to_index(X_dev_cat, word_idx)\n",
    "X_idx_test = to_index(X_test_cat, word_idx)\n",
    "Y_idx_dev = to_index(Y_dev_cat, ner_idx)\n",
    "Y_idx_test = to_index(Y_test_cat, ner_idx)\n",
    "print('First sentences, word indices', X_idx[:3])\n",
    "print('First sentences, NER indices', Y_idx[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3466, 150)\n",
      "(3684, 150)\n",
      "(14987, 150)\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0 935]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 9]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "max_len = 150\n",
    "X = pad_sequences(X_idx, maxlen = max_len)\n",
    "Y = pad_sequences(Y_idx, maxlen = max_len)\n",
    "X_dev = pad_sequences(X_idx_dev, maxlen = max_len)\n",
    "Y_dev = pad_sequences(Y_idx_dev,maxlen = max_len)\n",
    "X_test = pad_sequences(X_idx_test, maxlen = max_len)\n",
    "Y_test = pad_sequences(Y_idx_test,maxlen = max_len)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of NER classes and 0 (padding symbol)\n",
    "Y_train = to_categorical(Y, num_classes=len(ner) + 2)\n",
    "Y_val = to_categorical(Y_dev, num_classes=len(ner) + 2)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14987, 150)\n",
      "(14987, 150)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words) + 2, \n",
    "                                    EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402597, 100)\n",
      "Embedding of table [-0.61453998  0.89692998  0.56770998  0.39102    -0.22437     0.49035001\n",
      "  0.10868     0.27410999 -0.23833001 -0.52152997  0.73550999 -0.32653999\n",
      "  0.51304001  0.32415    -0.46709001  0.68050998 -0.25497001 -0.040484\n",
      " -0.54417998 -1.05480003 -0.46691999  0.23557     0.31233999 -0.34536999\n",
      "  0.14793    -0.53745002 -0.43215001 -0.48723999 -0.51019001 -0.90509999\n",
      " -0.17918999 -0.018376    0.09719    -0.31623     0.75120002  0.92236\n",
      " -0.49965     0.14036    -0.28296    -0.97443002 -0.0094408  -0.62944001\n",
      "  0.14711    -0.94375998  0.0075222   0.18565001 -0.99172002  0.072789\n",
      " -0.18474001 -0.52901     0.38995001 -0.45677    -0.21932     1.37230003\n",
      " -0.29635999 -2.2342     -0.36667001  0.04987     0.63420999  0.53275001\n",
      " -0.53955001  0.31398001 -0.44698    -0.38389     0.066668   -0.02168\n",
      "  0.20558     0.59456003 -0.24891999 -0.52794999 -0.3761      0.077104\n",
      "  0.75221997 -0.2647     -0.0587      0.67540997 -0.16559    -0.49278\n",
      " -0.26326999 -0.21214999  0.24316999  0.17005999 -0.29260001 -0.50089997\n",
      " -0.56638002 -0.40377    -0.48451999 -0.32539001  0.75292999  0.0049585\n",
      " -0.32115     0.28898999 -0.042392    0.63862997 -0.20332    -0.46785\n",
      " -0.15661     0.21789999  1.41429996  0.40033999]\n",
      "Embedding of the padding symbol, idx 0, random numbers [-0.02629708 -0.04923516 -0.04801697 -0.01869074 -0.04005453 -0.03048257\n",
      " -0.0292702  -0.03350688  0.0211879  -0.04679333 -0.03026304  0.0464557\n",
      "  0.00738946  0.01992277  0.04746414  0.01543505 -0.02391317 -0.03035904\n",
      "  0.03614633 -0.01292743 -0.01311645  0.00429138  0.01827985  0.03228761\n",
      " -0.03686076 -0.04223968  0.03409078 -0.0278994   0.02529113 -0.0156977\n",
      " -0.04902496 -0.01042922 -0.029072   -0.00319148 -0.01353996  0.00950514\n",
      "  0.0413734  -0.00028032 -0.01519774 -0.01369095  0.03702888 -0.01152137\n",
      "  0.03035301  0.00264644 -0.0463597  -0.02356203 -0.033484   -0.02621933\n",
      " -0.03773337  0.01826283  0.03646911 -0.04109766 -0.03953006  0.04822013\n",
      " -0.02821295 -0.0431476  -0.02476419  0.04927545 -0.02866612 -0.00881531\n",
      " -0.01183301  0.02965345 -0.03483367  0.0109977  -0.04514807 -0.0231921\n",
      "  0.00176915  0.00485835 -0.0040727  -0.01905112 -0.02361332 -0.03425079\n",
      "  0.04564135 -0.02310861  0.04121954 -0.04504611  0.00330172  0.01987282\n",
      " -0.00783856  0.0198199  -0.02306001 -0.0187176   0.03677814 -0.03901311\n",
      " -0.03016801  0.03832556  0.02892775  0.04970791 -0.01038987  0.04192337\n",
      "  0.0403074  -0.01217054  0.04221675 -0.02228818  0.03978376 -0.00845296\n",
      "  0.00691154  0.01943966  0.00093845 -0.03084958]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Embedding of table', embedding_matrix[word_idx['table']])\n",
    "print('Embedding of the padding symbol, idx 0, random numbers', \n",
    "      embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "retrainRNN = True\n",
    "Complex = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 150, 100)          20100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 150, 10)           1010      \n",
      "=================================================================\n",
      "Total params: 40,280,810\n",
      "Trainable params: 40,280,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelRNN = models.Sequential()\n",
    "modelRNN.add(layers.Embedding(len(vocabulary_words) + 2, EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "\n",
    "if Complex == True:\n",
    "    modelRNN.add(layers.Bidirectional(layers.SimpleRNN(32, return_sequences=True)))\n",
    "    modelRNN.add(layers.Bidirectional(layers.SimpleRNN(32, return_sequences=True)))\n",
    "    modelRNN.add(layers.Dropout(0.25))\n",
    "    modelRNN.add(layers.Bidirectional(layers.SimpleRNN(32, return_sequences=True)))\n",
    "    \n",
    "else: \n",
    "    modelRNN.add(layers.SimpleRNN(100,return_sequences=True))\n",
    "\n",
    "modelRNN.add(layers.Dense(NB_CLASSES + 2,activation = 'softmax'))\n",
    "\n",
    "modelRNN.layers[0].set_weights([embedding_matrix])\n",
    "modelRNN.layers[0].trainable = True\n",
    "\n",
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xre = np.reshape(X,(-1,1))\n",
    "# Yre = np.reshape(Y_train, (-1,Y_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David_000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "14987/14987 [==============================] - 257s 17ms/step - loss: 0.0225 - accuracy: 0.9279\n",
      "Epoch 2/2\n",
      "14987/14987 [==============================] - 259s 17ms/step - loss: 0.0122 - accuracy: 0.9586\n"
     ]
    }
   ],
   "source": [
    "if retrainRNN == True:\n",
    "    modelRNN.fit(X,Y_train,epochs=2, batch_size = 32)\n",
    "else:\n",
    "    if Complex == False:\n",
    "        modelRNN.load_weights('RNNW.model')\n",
    "    else:\n",
    "        modelRNN.load_weights('RRNWComp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 9s 2ms/step\n",
      "Test Loss:  0.014541754119874795 \n",
      "Test Accuracy:  0.9479920864105225\n"
     ]
    }
   ],
   "source": [
    "Ycat_test = to_categorical(Y_test, num_classes=len(ner) + 2)\n",
    "testloss, testacc = modelRNN.evaluate(X_test,Ycat_test)\n",
    "print(\"Test Loss: \", testloss, \"\\nTest Accuracy: \", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = modelRNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_num = []\n",
    "for sent_nbr, sent_ner_predictions in enumerate(predictions):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_test_cat[sent_nbr]):]]\n",
    "# print(ner_pred_num[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n",
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]\n",
    "print(ner_pred[-1:])\n",
    "print(Y_test_cat[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll_dictorizer import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted NER tags:  46666\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of predicted NER tags: \", len(ner_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_list = []\n",
    "for nerpred_seq in ner_pred:\n",
    "    for nerpred in nerpred_seq:\n",
    "        ner_pred_list.append(nerpred)\n",
    "\n",
    "i = 0\n",
    "for dicts in test_dict:\n",
    "    for one_dict in dicts:\n",
    "        one_dict['pner'] = ner_pred_list[i]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_p = column_names + ['pner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first dict in the test\n",
    "if Complex == False:  \n",
    "    save('outRNN.txt', test_dict, column_names_p)\n",
    "    if retrainRNN:\n",
    "        modelRNN.save_weights('RNNW.model')\n",
    "else:\n",
    "    save('outRNNComp.txt', test_dict[1:], column_names_p)\n",
    "    if retrainRNN:\n",
    "        modelRNN.save_weights('RNNWComp.model')\n",
    "retrainRNN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['form', 'ppos', 'pchunk', 'ner', 'pner']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 150, 100)          80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150, 10)           1010      \n",
      "=================================================================\n",
      "Total params: 40,341,110\n",
      "Trainable params: 40,341,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelLSTM = models.Sequential()\n",
    "modelLSTM.add(layers.Embedding(len(vocabulary_words) + 2, EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "\n",
    "# \n",
    "if Complex: \n",
    "    modelLSTM.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\n",
    "    modelLSTM.add(layers.Dropout(0.25))\n",
    "    modelLSTM.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\n",
    "    modelLSTM.add(layers.Dropout(0.25))\n",
    "else:\n",
    "    modelLSTM.add(layers.LSTM(100, return_sequences=True))\n",
    "    \n",
    "modelLSTM.add(layers.Dense(NB_CLASSES + 2,activation = 'softmax'))\n",
    "    \n",
    "modelLSTM.layers[0].set_weights([embedding_matrix])\n",
    "modelLSTM.layers[0].trainable = True\n",
    "\n",
    "modelLSTM.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David_000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "14987/14987 [==============================] - 228s 15ms/step - loss: 0.0334 - accuracy: 0.9235\n",
      "Epoch 2/2\n",
      "14987/14987 [==============================] - 219s 15ms/step - loss: 0.0157 - accuracy: 0.9599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27a70ef84c8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLSTM.fit(X,Y_train,epochs=2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 8s 2ms/step\n",
      "Test Loss:  0.017207037622107034 \n",
      "Test Accuracy:  0.952866792678833\n"
     ]
    }
   ],
   "source": [
    "Ycat_test = to_categorical(Y_test, num_classes=len(ner) + 2)\n",
    "testloss, testacc = modelLSTM.evaluate(X_test,Ycat_test)\n",
    "print(\"Test Loss: \", testloss, \"\\nTest Accuracy: \", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelLSTM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_num = []\n",
    "for sent_nbr, sent_ner_predictions in enumerate(predictions):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_test_cat[sent_nbr]):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n",
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]\n",
    "print(ner_pred[-1:])\n",
    "print(Y_test_cat[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_list = []\n",
    "for nerpred_seq in ner_pred:\n",
    "    for nerpred in nerpred_seq:\n",
    "        ner_pred_list.append(nerpred)\n",
    "\n",
    "i = 0\n",
    "for dicts in test_dict:\n",
    "    for one_dict in dicts:\n",
    "#         if i > 0:\n",
    "#             one_dict['pner'] = ner_pred_list[i]\n",
    "#         else:\n",
    "#             print(\"The first dict\", one_dict)\n",
    "        i += 1\n",
    "#         if i >= len(ner_pred_list):\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_dict)):\n",
    "    if len(test_dict[i]) != len(ner_pred[i]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
