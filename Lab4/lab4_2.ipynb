{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session 4 Edan 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GloVe embeddings 6B from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16197931522305201220\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3225262489\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11582744326432906548\n",
      "physical_device_desc: \"device: 0, name: GeForce 920M, pci bus id: 0000:08:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "fhttp://localhost:8889/notebooks/Documents/EDAN95/edan95/Lab4/lab4_2.ipynb#rom tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def readGloveEmbeddings(file):\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    word_dict = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embeddings = np.array([float(val) for val in splitLine[1:]])\n",
    "        word_dict[word] = embeddings\n",
    "    print(\"Number of words collected: \", len(word_dict))\n",
    "    f.close()\n",
    "    return word_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words collected:  400000\n"
     ]
    }
   ],
   "source": [
    "word_dict = readGloveEmbeddings('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of collecting embeddings given by Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words collected:  400000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "glove_dir = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4'\n",
    "embedding_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'UTF-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.strip().split()\n",
    "    word = values[0]\n",
    "    embeddings = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = embeddings\n",
    "f.close()\n",
    "print(\"Number of words collected: \", len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity # Raises error for 2D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as npl\n",
    "\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    sim = np.dot(a, b)/(npl.norm(a)*npl.norm(b))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top index:  [ 437 2389  927  241 7221 1801]  for word  table\n",
      "Top index:  [1035 1029  348  695 2975  387]  for word  france\n",
      "Top index:  [2640 2238 3817 3384 2819 2038]  for word  sweden\n"
     ]
    }
   ],
   "source": [
    "comparison_words = ['table', 'france', 'sweden']\n",
    "most_simular = {}\n",
    "\n",
    "for word in comparison_words:\n",
    "    sim = np.array([cos_sim(embedding_index[word], embedding_index[compare]) for compare in embedding_index.keys()])\n",
    "    top_idx = np.argpartition(sim, -6)[-6:]\n",
    "    print(\"Top index: \", top_idx[0:], \" for word \", word)\n",
    "    most_simular[word] = np.array([list(embedding_index.keys())[ind] for ind in top_idx[0:]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['side' 'bottom' 'room' 'place' 'tables' 'table']\n",
      "['paris' 'spain' 'french' 'britain' 'belgium' 'france']\n",
      "['austria' 'netherlands' 'finland' 'denmark' 'norway' 'sweden']\n"
     ]
    }
   ],
   "source": [
    "print(most_simular['table'])\n",
    "print(most_simular['france'])\n",
    "print(most_simular['sweden'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Corpus and Collecting Building Indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4'\n",
    "\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + '\\eng.train'\n",
    "    dev_file = BASE_DIR + '\\eng.valid'\n",
    "    test_file = BASE_DIR + '\\eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence, words ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "Second sentence, NER ['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "X_train_cat, Y_train_cat = build_sequences(train_dict)\n",
    "X_dev_cat, Y_dev_cat = build_sequences(dev_dict)\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "print('Second sentence, words', X_train_cat[1])\n",
    "print('Second sentence, NER', Y_train_cat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_words = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_train_cat for word in sentence])))\n",
    "ner = sorted(list(set([ner for sentence \n",
    "                       in Y_train_cat for ner in sentence])))\n",
    "print(ner)\n",
    "NB_CLASSES = len(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file,encoding='UTF-8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in GloVe: 400000\n",
      "# unique words in the vocabulary: embeddings and corpus: 402597\n"
     ]
    }
   ],
   "source": [
    "embedding_file = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4\\glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)\n",
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "print('# unique words in the vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or NER lists) to indexes\n",
    "    :param X: List of word (or NER) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: [('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11)]\n",
      "NER index: [('B-LOC', 2), ('B-MISC', 3), ('B-ORG', 4), ('I-LOC', 5), ('I-MISC', 6), ('I-ORG', 7), ('I-PER', 8), ('O', 9)]\n",
      "First sentences, word indices [[935], [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936], [284434, 79019]]\n",
      "First sentences, NER indices [[9], [7, 9, 6, 9, 9, 9, 6, 9, 9], [8, 8]]\n"
     ]
    }
   ],
   "source": [
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "print('word index:', list(word_idx.items())[:10])\n",
    "print('NER index:', list(ner_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, ner_idx)\n",
    "X_idx_dev = to_index(X_dev_cat, word_idx)\n",
    "X_idx_test = to_index(X_test_cat, word_idx)\n",
    "Y_idx_dev = to_index(Y_dev_cat, ner_idx)\n",
    "Y_idx_test = to_index(Y_test_cat, ner_idx)\n",
    "print('First sentences, word indices', X_idx[:3])\n",
    "print('First sentences, NER indices', Y_idx[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3466, 150)\n",
      "(3684, 150)\n",
      "(14987, 150)\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0 935]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 9]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "max_len = 150\n",
    "X = pad_sequences(X_idx, maxlen = max_len)\n",
    "Y = pad_sequences(Y_idx, maxlen = max_len)\n",
    "X_dev = pad_sequences(X_idx_dev, maxlen = max_len)\n",
    "Y_dev = pad_sequences(Y_idx_dev,maxlen = max_len)\n",
    "X_test = pad_sequences(X_idx_test, maxlen = max_len)\n",
    "Y_test = pad_sequences(Y_idx_test,maxlen = max_len)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of NER classes and 0 (padding symbol)\n",
    "Y_train = to_categorical(Y, num_classes=len(ner) + 2)\n",
    "Y_val = to_categorical(Y_dev, num_classes=len(ner) + 2)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14987, 150)\n",
      "(3466, 150, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words) + 2, \n",
    "                                    EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402597, 100)\n",
      "Embedding of table [-0.61453998  0.89692998  0.56770998  0.39102    -0.22437     0.49035001\n",
      "  0.10868     0.27410999 -0.23833001 -0.52152997  0.73550999 -0.32653999\n",
      "  0.51304001  0.32415    -0.46709001  0.68050998 -0.25497001 -0.040484\n",
      " -0.54417998 -1.05480003 -0.46691999  0.23557     0.31233999 -0.34536999\n",
      "  0.14793    -0.53745002 -0.43215001 -0.48723999 -0.51019001 -0.90509999\n",
      " -0.17918999 -0.018376    0.09719    -0.31623     0.75120002  0.92236\n",
      " -0.49965     0.14036    -0.28296    -0.97443002 -0.0094408  -0.62944001\n",
      "  0.14711    -0.94375998  0.0075222   0.18565001 -0.99172002  0.072789\n",
      " -0.18474001 -0.52901     0.38995001 -0.45677    -0.21932     1.37230003\n",
      " -0.29635999 -2.2342     -0.36667001  0.04987     0.63420999  0.53275001\n",
      " -0.53955001  0.31398001 -0.44698    -0.38389     0.066668   -0.02168\n",
      "  0.20558     0.59456003 -0.24891999 -0.52794999 -0.3761      0.077104\n",
      "  0.75221997 -0.2647     -0.0587      0.67540997 -0.16559    -0.49278\n",
      " -0.26326999 -0.21214999  0.24316999  0.17005999 -0.29260001 -0.50089997\n",
      " -0.56638002 -0.40377    -0.48451999 -0.32539001  0.75292999  0.0049585\n",
      " -0.32115     0.28898999 -0.042392    0.63862997 -0.20332    -0.46785\n",
      " -0.15661     0.21789999  1.41429996  0.40033999]\n",
      "Embedding of the padding symbol, idx 0, random numbers [-0.02629708 -0.04923516 -0.04801697 -0.01869074 -0.04005453 -0.03048257\n",
      " -0.0292702  -0.03350688  0.0211879  -0.04679333 -0.03026304  0.0464557\n",
      "  0.00738946  0.01992277  0.04746414  0.01543505 -0.02391317 -0.03035904\n",
      "  0.03614633 -0.01292743 -0.01311645  0.00429138  0.01827985  0.03228761\n",
      " -0.03686076 -0.04223968  0.03409078 -0.0278994   0.02529113 -0.0156977\n",
      " -0.04902496 -0.01042922 -0.029072   -0.00319148 -0.01353996  0.00950514\n",
      "  0.0413734  -0.00028032 -0.01519774 -0.01369095  0.03702888 -0.01152137\n",
      "  0.03035301  0.00264644 -0.0463597  -0.02356203 -0.033484   -0.02621933\n",
      " -0.03773337  0.01826283  0.03646911 -0.04109766 -0.03953006  0.04822013\n",
      " -0.02821295 -0.0431476  -0.02476419  0.04927545 -0.02866612 -0.00881531\n",
      " -0.01183301  0.02965345 -0.03483367  0.0109977  -0.04514807 -0.0231921\n",
      "  0.00176915  0.00485835 -0.0040727  -0.01905112 -0.02361332 -0.03425079\n",
      "  0.04564135 -0.02310861  0.04121954 -0.04504611  0.00330172  0.01987282\n",
      " -0.00783856  0.0198199  -0.02306001 -0.0187176   0.03677814 -0.03901311\n",
      " -0.03016801  0.03832556  0.02892775  0.04970791 -0.01038987  0.04192337\n",
      "  0.0403074  -0.01217054  0.04221675 -0.02228818  0.03978376 -0.00845296\n",
      "  0.00691154  0.01943966  0.00093845 -0.03084958]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Embedding of table', embedding_matrix[word_idx['table']])\n",
    "print('Embedding of the padding symbol, idx 0, random numbers', \n",
    "      embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file, corpus_dict, column_names):\n",
    "    with open(file, 'w', newline='\\n') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = list(map(lambda x: row.get(x, '_'), column_names))\n",
    "                sentence_lst += [str(tag) + \" \" for tag in items[:-1]]\n",
    "                sentence_lst += str(items[-1])\n",
    "                sentence_lst += '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "retrainRNN = True\n",
    "Complex = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 150, 200)          40200     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 150, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 150, 10)           2010      \n",
      "=================================================================\n",
      "Total params: 40,301,910\n",
      "Trainable params: 40,301,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelRNN = models.Sequential()\n",
    "modelRNN.add(layers.Embedding(len(vocabulary_words) + 2, EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "\n",
    "if Complex == True:\n",
    "    modelRNN.add(layers.Bidirectional(layers.SimpleRNN(100, return_sequences=True)))\n",
    "    modelRNN.add(layers.Dropout(0.25))\n",
    "    \n",
    "else: \n",
    "    modelRNN.add(layers.SimpleRNN(100,return_sequences=True))\n",
    "\n",
    "modelRNN.add(layers.Dense(NB_CLASSES + 2,activation = 'softmax'))\n",
    "\n",
    "modelRNN.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRNN.layers[0].trainable = True\n",
    "modelRNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David_000\\Anaconda3\\envs\\TensorFlowGPU\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 115s 8ms/step - loss: 0.0534 - accuracy: 0.8327 - val_loss: 0.0288 - val_accuracy: 0.9147\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 112s 7ms/step - loss: 0.0224 - accuracy: 0.9271 - val_loss: 0.0196 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 109s 7ms/step - loss: 0.0159 - accuracy: 0.9481 - val_loss: 0.0159 - val_accuracy: 0.9540\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 118s 8ms/step - loss: 0.0124 - accuracy: 0.9598 - val_loss: 0.0139 - val_accuracy: 0.9606\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 120s 8ms/step - loss: 0.0100 - accuracy: 0.9676 - val_loss: 0.0126 - val_accuracy: 0.9637\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 112s 7ms/step - loss: 0.0082 - accuracy: 0.9733 - val_loss: 0.0119 - val_accuracy: 0.9658\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 117s 8ms/step - loss: 0.0069 - accuracy: 0.9776 - val_loss: 0.0113 - val_accuracy: 0.9670\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 109s 7ms/step - loss: 0.0058 - accuracy: 0.9811 - val_loss: 0.0110 - val_accuracy: 0.9687\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 112s 7ms/step - loss: 0.0049 - accuracy: 0.9844 - val_loss: 0.0108 - val_accuracy: 0.9691\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 109s 7ms/step - loss: 0.0041 - accuracy: 0.9870 - val_loss: 0.0109 - val_accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "if retrainRNN == True:\n",
    "    historyRNN = modelRNN.fit(X,Y_train,epochs=10, batch_size = 256, validation_data=(X_dev,Y_val))\n",
    "else:\n",
    "    if Complex == False:\n",
    "        modelRNN.load_weights('RNNW.model')\n",
    "    else:\n",
    "        modelRNN.load_weights('RRNWComp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 20s 5ms/step\n",
      "Test Loss:  0.013217231106689558 \n",
      "Test Accuracy:  0.9560065269470215\n"
     ]
    }
   ],
   "source": [
    "Ycat_test = to_categorical(Y_test, num_classes=len(ner) + 2)\n",
    "testloss, testacc = modelRNN.evaluate(X_test,Ycat_test)\n",
    "print(\"Test Loss: \", testloss, \"\\nTest Accuracy: \", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = modelRNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_num = []\n",
    "for sent_nbr, sent_ner_predictions in enumerate(predictions):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_test_cat[sent_nbr]):]]\n",
    "# print(ner_pred_num[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n",
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]\n",
    "print(ner_pred[-1:])\n",
    "print(Y_test_cat[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_list = []\n",
    "for nerpred_seq in ner_pred:\n",
    "    for nerpred in nerpred_seq:\n",
    "        ner_pred_list.append(nerpred)\n",
    "\n",
    "i = 0\n",
    "for dicts in test_dict:\n",
    "    for one_dict in dicts:\n",
    "        one_dict['pner'] = ner_pred_list[i]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_p = ['form', 'ner', 'pner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first dict in the test\n",
    "if Complex == False:  \n",
    "    save('outRNN.txt', test_dict, column_names_p)\n",
    "    if retrainRNN:\n",
    "        modelRNN.save_weights('RNN.model')\n",
    "else:\n",
    "    save('outRNNComp.txt', test_dict, column_names_p)\n",
    "    if retrainRNN:\n",
    "        modelRNN.save_weights('RNNComp.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !perl conlleval -d '\\t' <outRNN.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrainLSTM = True\n",
    "Complex = False\n",
    "from keras import models, layers\n",
    "import keras.backend as K\n",
    "# from keras_contrib.layers import CRF\n",
    "# from keras_contrib.losses import crf_loss\n",
    "# from keras_contrib.metrics import crf_marginal_accuracy\n",
    "# from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "# !pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y),\n",
    "                                                 np.reshape(Y,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "model = layers.Embedding(input_dim = len(vocabulary_words) + 2, output_dim=100,\n",
    "                  input_length=max_len, mask_zero=True)(input)  # 20-dim embedding\n",
    "model = layers.Bidirectional(layers.LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = layers.TimeDistributed(layers.Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(NB_CLASSES+2)  # CRF layer\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 150, 100)          80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150, 10)           1010      \n",
      "=================================================================\n",
      "Total params: 40,341,110\n",
      "Trainable params: 40,341,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelLSTM = models.Sequential()\n",
    "modelLSTM.add(layers.Embedding(len(vocabulary_words) + 2, EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "\n",
    "# \n",
    "if Complex: \n",
    "    modelLSTM.add(layers.Bidirectional(layers.LSTM(100, return_sequences=True)))\n",
    "    modelLSTM.add(layers.Dropout(0.50))\n",
    "#     modelLSTM.add(layers.Bidirectional(layers.LSTM(100, return_sequences=True)))\n",
    "#     modelLSTM.add(layers.Dropout(0.50))\n",
    "    modelLSTM.add(layers.TimeDistributed(layers.Dense(100, activation=\"relu\")))\n",
    "#     modelLSTM.add(layers.LSTM(50, return_sequences=True, recurrent_dropout = 0.1))\n",
    "#     modelLSTM.add(layers.Bidirectional(layers.LSTM(50, return_sequences=True, recurrent_dropout = 0.1)))\n",
    "else:\n",
    "    modelLSTM.add(layers.LSTM(100, return_sequences=True))\n",
    "    \n",
    "modelLSTM.add(layers.Dense(NB_CLASSES + 2,activation = 'softmax'))\n",
    "# crf = CRF(NB_CLASSES+2,sparse_target = True)\n",
    "# modelLSTM.add(crf)\n",
    "    \n",
    "modelLSTM.layers[0].set_weights([embedding_matrix])\n",
    "# modelLSTM.layers[0].trainable = True\n",
    "\n",
    "modelLSTM.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.layers[0].trainable = True\n",
    "modelLSTM.compile(loss='categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David_000\\Anaconda3\\envs\\TensorFlowGPU\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/2\n",
      "14987/14987 [==============================] - 224s 15ms/step - loss: 0.0403 - accuracy: 0.8881 - val_loss: 0.0203 - val_accuracy: 0.9459\n",
      "Epoch 2/2\n",
      "14987/14987 [==============================] - 212s 14ms/step - loss: 0.0131 - accuracy: 0.9597 - val_loss: 0.0183 - val_accuracy: 0.9524\n",
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/6\n",
      "14987/14987 [==============================] - 119s 8ms/step - loss: 0.0086 - accuracy: 0.9719 - val_loss: 0.0137 - val_accuracy: 0.9619\n",
      "Epoch 2/6\n",
      "14987/14987 [==============================] - 121s 8ms/step - loss: 0.0075 - accuracy: 0.9749 - val_loss: 0.0130 - val_accuracy: 0.9634\n",
      "Epoch 3/6\n",
      "14987/14987 [==============================] - 83s 6ms/step - loss: 0.0068 - accuracy: 0.9767 - val_loss: 0.0156 - val_accuracy: 0.9472\n",
      "Epoch 4/6\n",
      "14987/14987 [==============================] - 147s 10ms/step - loss: 0.0063 - accuracy: 0.9780 - val_loss: 0.0134 - val_accuracy: 0.9643\n",
      "Epoch 5/6\n",
      "14987/14987 [==============================] - 107s 7ms/step - loss: 0.0058 - accuracy: 0.9794 - val_loss: 0.0123 - val_accuracy: 0.9620\n",
      "Epoch 6/6\n",
      "14987/14987 [==============================] - 79s 5ms/step - loss: 0.0054 - accuracy: 0.9805 - val_loss: 0.0119 - val_accuracy: 0.9661\n"
     ]
    }
   ],
   "source": [
    "# class_weight = np.ones(NB_CLASSES)*0.9\n",
    "# class_weight[-1] = 0.1\n",
    "if retrainLSTM:\n",
    "    historyLSTM = modelLSTM.fit(X,Y_train,epochs=2,\n",
    "                                batch_size = 128, \n",
    "                                validation_data=(X_dev,Y_val))\n",
    "    \n",
    "    modelLSTM.layers[0].trainable = False\n",
    "    modelLSTM.compile(loss='categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    historyLSTM = modelLSTM.fit(X,Y_train,epochs=6,\n",
    "                                batch_size = 128, \n",
    "                                validation_data=(X_dev,Y_val))\n",
    "else:\n",
    "    if Complex:\n",
    "        modelLSTM.load_weights(\"LSTMComp.model\")\n",
    "    else:\n",
    "        modelLSTM.load_weights(\"LSTM.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to add to the network:\n",
    " - Stack LSTM layers instead of creating a huge one since they each need quadratic number of weights\n",
    " - Experiment with adding dropout\n",
    " - Add class weights since the class distribution is skewed\n",
    " - Implement CRF layer and Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 32s 9ms/step\n",
      "Test Loss:  0.013793187600941711 \n",
      "Test Accuracy:  0.9525350332260132\n"
     ]
    }
   ],
   "source": [
    "Ycat_test = to_categorical(Y_test, num_classes=len(ner) + 2)\n",
    "testloss, testacc = modelLSTM.evaluate(X_test,Ycat_test)\n",
    "print(\"Test Loss: \", testloss, \"\\nTest Accuracy: \", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelLSTM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_num = []\n",
    "for sent_nbr, sent_ner_predictions in enumerate(predictions):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_test_cat[sent_nbr]):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n",
      "[['O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O']]\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]\n",
    "print(ner_pred[-1:])\n",
    "print(Y_test_cat[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_list = []\n",
    "for nerpred_seq in ner_pred:\n",
    "    for nerpred in nerpred_seq:\n",
    "        ner_pred_list.append(nerpred)\n",
    "\n",
    "i = 0\n",
    "for dicts in test_dict:\n",
    "    for one_dict in dicts:\n",
    "        one_dict['pner'] = ner_pred_list[i]\n",
    "        i += 1\n",
    "#         if i >= len(ner_pred_list):\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_p = ['form', 'ner', 'pner']\n",
    "Complex = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first dict in the test\n",
    "if Complex == False:  \n",
    "    save('outLSTM.txt', test_dict, column_names_p)\n",
    "#     if retrainLSTM:\n",
    "#         modelLSTM.save_weights('LSTM.model')\n",
    "else:\n",
    "    save('outLSTMComp.txt', test_dict, column_names_p)\n",
    "#     if retrainLSTM:\n",
    "#         modelLSTM.save_weights('LSTMComp.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/2\n",
      "14987/14987 [==============================] - 185s 12ms/step - loss: 0.0011 - accuracy: 0.9958 - val_loss: 0.0114 - val_accuracy: 0.9762\n",
      "Epoch 2/2\n",
      "14987/14987 [==============================] - 149s 10ms/step - loss: 0.0011 - accuracy: 0.9960 - val_loss: 0.0114 - val_accuracy: 0.9762\n"
     ]
    }
   ],
   "source": [
    "K.set_value(modelLSTM.optimizer.lr, 0.000001)\n",
    "historyLSTM = modelLSTM.fit(X,Y_train,epochs=2,\n",
    "                                batch_size = 128, \n",
    "                                validation_data=(X_dev,Y_val),\n",
    "                           class_weight = class_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
