{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session 4 Edan 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GloVe embeddings 6B from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def readGloveEmbeddings(file):\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    word_dict = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embeddings = np.array([float(val) for val in splitLine[1:]])\n",
    "        word_dict[word] = embeddings\n",
    "    print(\"Number of words collected: \", len(word_dict))\n",
    "    f.close()\n",
    "    return word_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words collected:  400000\n"
     ]
    }
   ],
   "source": [
    "word_dict = readGloveEmbeddings('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of collecting embeddings given by Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words collected:  400000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "glove_dir = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4'\n",
    "embedding_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'UTF-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.strip().split()\n",
    "    word = values[0]\n",
    "    embeddings = np.array(values[1:], dtype='float32')\n",
    "    embedding_index[word] = embeddings\n",
    "f.close()\n",
    "print(\"Number of words collected: \", len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity # Raises error for 2D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as npl\n",
    "\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    sim = np.dot(a, b)/(npl.norm(a)*npl.norm(b))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top index:  [ 437 2389  927  241 7221 1801]  for word  table\n",
      "Top index:  [1035 1029  348  695 2975  387]  for word  france\n",
      "Top index:  [2640 2238 3817 3384 2819 2038]  for word  sweden\n"
     ]
    }
   ],
   "source": [
    "comparison_words = ['table', 'france', 'sweden']\n",
    "most_simular = {}\n",
    "\n",
    "for word in comparison_words:\n",
    "    sim = np.array([cos_sim(embedding_index[word], embedding_index[compare]) for compare in embedding_index.keys()])\n",
    "    top_idx = np.argpartition(sim, -6)[-6:]\n",
    "    print(\"Top index: \", top_idx[0:], \" for word \", word)\n",
    "    most_simular[word] = np.array([list(embedding_index.keys())[ind] for ind in top_idx[0:]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['side' 'bottom' 'room' 'place' 'tables' 'table']\n",
      "['paris' 'spain' 'french' 'britain' 'belgium' 'france']\n",
      "['austria' 'netherlands' 'finland' 'denmark' 'norway' 'sweden']\n"
     ]
    }
   ],
   "source": [
    "print(most_simular['table'])\n",
    "print(most_simular['france'])\n",
    "print(most_simular['sweden'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Corpus and Collecting Building Indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4'\n",
    "\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + '\\eng.train'\n",
    "    dev_file = BASE_DIR + '\\eng.valid'\n",
    "    test_file = BASE_DIR + '\\eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'I-NP', 'ner': 'I-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words ['-docstart-']\n",
      "First sentence, NER ['O']\n"
     ]
    }
   ],
   "source": [
    "X_train_cat, Y_train_cat = build_sequences(train_dict)\n",
    "X_dev_cat, Y_dev_cat = build_sequences(dev_dict)\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "print('First sentence, words', X_train_cat[0])\n",
    "print('First sentence, NER', Y_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_words = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_train_cat for word in sentence])))\n",
    "ner = sorted(list(set([ner for sentence \n",
    "                       in Y_train_cat for ner in sentence])))\n",
    "print(ner)\n",
    "NB_CLASSES = len(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file,encoding='UTF-8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in GloVe: 400000\n",
      "# unique words in the vocabulary: embeddings and corpus: 402597\n"
     ]
    }
   ],
   "source": [
    "embedding_file = r'C:\\Users\\David_000\\Documents\\EDAN95\\edan95\\Lab4\\glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)\n",
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "print('# unique words in the vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or NER lists) to indexes\n",
    "    :param X: List of word (or NER) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: [('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11)]\n",
      "NER index: [('B-LOC', 2), ('B-MISC', 3), ('B-ORG', 4), ('I-LOC', 5), ('I-MISC', 6), ('I-ORG', 7), ('I-PER', 8), ('O', 9)]\n",
      "First sentences, word indices [[935], [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936], [284434, 79019]]\n",
      "First sentences, NER indices [[9], [7, 9, 6, 9, 9, 9, 6, 9, 9], [8, 8]]\n"
     ]
    }
   ],
   "source": [
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "print('word index:', list(word_idx.items())[:10])\n",
    "print('NER index:', list(ner_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, ner_idx)\n",
    "X_idx_dev = to_index(X_dev_cat, word_idx)\n",
    "X_idx_test = to_index(X_test_cat, word_idx)\n",
    "Y_idx_dev = to_index(Y_dev_cat, ner_idx)\n",
    "Y_idx_test = to_index(Y_test_cat, ner_idx)\n",
    "print('First sentences, word indices', X_idx[:3])\n",
    "print('First sentences, NER indices', Y_idx[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0 935]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 9]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X_idx)\n",
    "Y = pad_sequences(Y_idx)\n",
    "max_len = X.shape[1]\n",
    "X_dev = pad_sequences(X_idx_dev,maxlen = max_len)\n",
    "Y_dev = pad_sequences(Y_idx_dev,maxlen = max_len)\n",
    "X_test = pad_sequences(X_idx_test,maxlen = max_len)\n",
    "Y_test = pad_sequences(Y_idx_test,maxlen = max_len)\n",
    "\n",
    "max_len = X.shape[1]\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of NER classes and 0 (padding symbol)\n",
    "Y_train = to_categorical(Y, num_classes=len(ner) + 2)\n",
    "Y_val = to_categorical(Y_dev, num_classes=len(ner) + 2)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14987, 113)\n",
      "(14987, 113)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words) + 2, \n",
    "                                    EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402597, 100)\n",
      "Embedding of table [-0.61453998  0.89692998  0.56770998  0.39102    -0.22437     0.49035001\n",
      "  0.10868     0.27410999 -0.23833001 -0.52152997  0.73550999 -0.32653999\n",
      "  0.51304001  0.32415    -0.46709001  0.68050998 -0.25497001 -0.040484\n",
      " -0.54417998 -1.05480003 -0.46691999  0.23557     0.31233999 -0.34536999\n",
      "  0.14793    -0.53745002 -0.43215001 -0.48723999 -0.51019001 -0.90509999\n",
      " -0.17918999 -0.018376    0.09719    -0.31623     0.75120002  0.92236\n",
      " -0.49965     0.14036    -0.28296    -0.97443002 -0.0094408  -0.62944001\n",
      "  0.14711    -0.94375998  0.0075222   0.18565001 -0.99172002  0.072789\n",
      " -0.18474001 -0.52901     0.38995001 -0.45677    -0.21932     1.37230003\n",
      " -0.29635999 -2.2342     -0.36667001  0.04987     0.63420999  0.53275001\n",
      " -0.53955001  0.31398001 -0.44698    -0.38389     0.066668   -0.02168\n",
      "  0.20558     0.59456003 -0.24891999 -0.52794999 -0.3761      0.077104\n",
      "  0.75221997 -0.2647     -0.0587      0.67540997 -0.16559    -0.49278\n",
      " -0.26326999 -0.21214999  0.24316999  0.17005999 -0.29260001 -0.50089997\n",
      " -0.56638002 -0.40377    -0.48451999 -0.32539001  0.75292999  0.0049585\n",
      " -0.32115     0.28898999 -0.042392    0.63862997 -0.20332    -0.46785\n",
      " -0.15661     0.21789999  1.41429996  0.40033999]\n",
      "Embedding of the padding symbol, idx 0, random numbers [-0.02629708 -0.04923516 -0.04801697 -0.01869074 -0.04005453 -0.03048257\n",
      " -0.0292702  -0.03350688  0.0211879  -0.04679333 -0.03026304  0.0464557\n",
      "  0.00738946  0.01992277  0.04746414  0.01543505 -0.02391317 -0.03035904\n",
      "  0.03614633 -0.01292743 -0.01311645  0.00429138  0.01827985  0.03228761\n",
      " -0.03686076 -0.04223968  0.03409078 -0.0278994   0.02529113 -0.0156977\n",
      " -0.04902496 -0.01042922 -0.029072   -0.00319148 -0.01353996  0.00950514\n",
      "  0.0413734  -0.00028032 -0.01519774 -0.01369095  0.03702888 -0.01152137\n",
      "  0.03035301  0.00264644 -0.0463597  -0.02356203 -0.033484   -0.02621933\n",
      " -0.03773337  0.01826283  0.03646911 -0.04109766 -0.03953006  0.04822013\n",
      " -0.02821295 -0.0431476  -0.02476419  0.04927545 -0.02866612 -0.00881531\n",
      " -0.01183301  0.02965345 -0.03483367  0.0109977  -0.04514807 -0.0231921\n",
      "  0.00176915  0.00485835 -0.0040727  -0.01905112 -0.02361332 -0.03425079\n",
      "  0.04564135 -0.02310861  0.04121954 -0.04504611  0.00330172  0.01987282\n",
      " -0.00783856  0.0198199  -0.02306001 -0.0187176   0.03677814 -0.03901311\n",
      " -0.03016801  0.03832556  0.02892775  0.04970791 -0.01038987  0.04192337\n",
      "  0.0403074  -0.01217054  0.04221675 -0.02228818  0.03978376 -0.00845296\n",
      "  0.00691154  0.01943966  0.00093845 -0.03084958]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Embedding of table', embedding_matrix[word_idx['table']])\n",
    "print('Embedding of the padding symbol, idx 0, random numbers', \n",
    "      embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 113, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 113, 100)          20100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 113, 10)           1010      \n",
      "=================================================================\n",
      "Total params: 40,280,810\n",
      "Trainable params: 40,280,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(vocabulary_words) + 2, EMBEDDING_DIM, input_length=max_len, mask_zero=True))\n",
    "model.add(layers.SimpleRNN(100,return_sequences=True))\n",
    "model.add(layers.Dense(NB_CLASSES + 2,activation = 'sigmoid'))\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = True\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xre = np.reshape(X,(-1,1))\n",
    "# Yre = np.reshape(Y_train, (-1,Y_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "14987/14987 [==============================] - 215s 14ms/step - loss: 0.0137 - accuracy: 0.9649\n",
      "Epoch 2/2\n",
      "14987/14987 [==============================] - 235s 16ms/step - loss: 0.0117 - accuracy: 0.9696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2bcecd68e48>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y_train,epochs=2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 6s 2ms/step\n",
      "Test Loss:  0.01834092480024317 \n",
      "Test Accuracy:  0.9485585689544678\n"
     ]
    }
   ],
   "source": [
    "Ycat_test = to_categorical(Y_test, num_classes=len(ner) + 2)\n",
    "testloss, testacc = model.evaluate(X_test,Ycat_test)\n",
    "print(\"Test Loss: \", testloss, \"\\nTest Accuracy: \", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 9, 9, ..., 9, 9, 9],\n",
       "       [9, 9, 9, ..., 9, 9, 9],\n",
       "       [9, 9, 9, ..., 9, 8, 8],\n",
       "       ...,\n",
       "       [9, 9, 9, ..., 9, 9, 9],\n",
       "       [9, 9, 9, ..., 9, 9, 9],\n",
       "       [9, 9, 9, ..., 9, 8, 9]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 9],\n",
       "       [0, 0, 0, ..., 9, 9, 9],\n",
       "       [0, 0, 0, ..., 0, 8, 8],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 9, 9, 9],\n",
       "       [0, 0, 0, ..., 9, 9, 9],\n",
       "       [0, 0, 0, ..., 9, 8, 9]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
